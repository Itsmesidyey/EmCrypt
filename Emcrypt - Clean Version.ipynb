{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2a48e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#SpellCorrection\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import string\n",
    "import emoji\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7ec4dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: UTF-8-SIG\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>9:40 PM · Oct 24, 2023</td>\n",
       "      <td>@CryptoSpac3</td>\n",
       "      <td>Get ready for the next #bullrun in the #crypto...</td>\n",
       "      <td>2</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>10/29/23</td>\n",
       "      <td>@azami_iqbal2007</td>\n",
       "      <td>🚀Discover #SaitamaToken: The Future of #Crypto...</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>10/29/23</td>\n",
       "      <td>@neilstucky1</td>\n",
       "      <td>For those in $algo governance, enjoy the massi...</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>12:22 AM · Oct 26, 2023</td>\n",
       "      <td>@0x0Checkmate</td>\n",
       "      <td>🚀 #ShibaInu continues its upward trajectory as...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>10/28/23</td>\n",
       "      <td>@univaultx</td>\n",
       "      <td>🚀Exciting news! UNIVAULT is teaming up with @M...</td>\n",
       "      <td>2</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date          username  \\\n",
       "121   9:40 PM · Oct 24, 2023      @CryptoSpac3   \n",
       "424                 10/29/23  @azami_iqbal2007   \n",
       "444                 10/29/23      @neilstucky1   \n",
       "54   12:22 AM · Oct 26, 2023     @0x0Checkmate   \n",
       "381                 10/28/23        @univaultx   \n",
       "\n",
       "                                                  text  polarity       emotion  \n",
       "121  Get ready for the next #bullrun in the #crypto...         2  anticipation  \n",
       "424  🚀Discover #SaitamaToken: The Future of #Crypto...         1         happy  \n",
       "444  For those in $algo governance, enjoy the massi...         1         happy  \n",
       "54   🚀 #ShibaInu continues its upward trajectory as...         2         happy  \n",
       "381  🚀Exciting news! UNIVAULT is teaming up with @M...         2  anticipation  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['date', 'username', 'text', 'polarity', 'emotion']\n",
    "\n",
    "#Detect file encoding using chardet\n",
    "with open('data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Print the detected encoding\n",
    "print(\"Detected encoding:\", result['encoding'])\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "df = pd.read_csv('data.csv', encoding=result['encoding'], names=DATASET_COLUMNS)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "10f5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "data=df[['text','polarity', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ed21aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2ba51c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['polarity'] == 2]\n",
    "data_neu = data[data['polarity'] == 1]\n",
    "data_neg = data[data['polarity'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6d97b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(200)]\n",
    "data_neu = data_neu.iloc[:int(200)]\n",
    "data_neg = data_neg.iloc[:int(200)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0888a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([data_pos, data_neu, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b8bd68dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582     Which #crypto project has strong community? 💪🏼🔥🚀\n",
       "584    New Zealand, Rapper Sesh and DogeCoin Milliona...\n",
       "585    The founder of the bankrupt cryptocurrency exc...\n",
       "586    Unlock the Future with .mmit Domains! Join ove...\n",
       "595    If you sleep now, you will have a dream but if...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3469fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1            HAY bullflag breakout👀 Lets fill that wick🚀\n",
      "2      Did you guys see how is doing a pitch with a d...\n",
      "3      GN Fam going early to bed been up since or AM ...\n",
      "4      You think this week has been fun?!? 😂😂😂😂 Wait ...\n",
      "                             ...                        \n",
      "582                Which project has strong community? 🚀\n",
      "584    New Zealand Rapper Sesh and DogeCoin Millionai...\n",
      "585    The founder of the bankrupt cryptocurrency exc...\n",
      "586    Unlock the Future with .mmit Domains! Join ove...\n",
      "595    If you sleep now you will have a dream but if ...\n",
      "Name: text, Length: 347, dtype: object\n"
     ]
    }
   ],
   "source": [
    "emoticons_to_keep = [\n",
    "    '💰', '📈', '🤣', '🎊', '😂', '😭', '🙁', '😞', '💔', '😢', '😮', '😵', '🙀',\n",
    "    '😱', '❗', '😠', '😡', '😤', '👎', '🔪', '🌕', '🚀', '💎', '👀', '💭', '📉',\n",
    "    '😨', '😩', '😰', '💸'\n",
    "]\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove special characters except for emoticons\n",
    "    text = re.sub(r'[^\\w\\s.!?{}]+'.format(''.join(emoticons_to_keep)), '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the modified cleaning function to the 'text' column in your dataset\n",
    "dataset['text'] = dataset['text'].apply(clean_tweet)\n",
    "\n",
    "# Display the 'text' column in the entire dataset\n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9db4387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  polarity       emotion\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...         2         happy\n",
      "1            HAY bullfrog breakout Lets fill that wick         2  anticipation\n",
      "2    Did you guys see how is doing a pitch with a d...         2         happy\n",
      "3    GN Fam going early to bed been up since or AM ...         2         happy\n",
      "4    You think this week has been fun?!? 😂😂😂😂 Wait ...         2  anticipation\n",
      "..                                                 ...       ...           ...\n",
      "582               Which project has strong community i         1         happy\n",
      "584  New Zealand Rapper Sesh and DogeCoin Millionai...         1         happy\n",
      "585  The founder of the bankrupt cryptocurrency exc...         1         happy\n",
      "586  Unlock the Future with emmit Domains! Join ove...         1         happy\n",
      "595  If you sleep now you will have a dream but if ...         1         happy\n",
      "\n",
      "[347 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SpellChecker only once to avoid re-creation for each call\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function for spell correction\n",
    "def spell_correction(text):\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Check if the correction is not None, otherwise use the original word\n",
    "            corrected_words.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply spell correction to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(spell_correction)\n",
    "\n",
    "# Display the entire dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52520eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons converted to words in 'converted_text' column.\n",
      "                                      converted_text  emoticons_count\n",
      "0  BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...                0\n",
      "1          HAY bullfrog breakout Lets fill that wick                0\n",
      "2  Did you guys see how is doing a pitch with a d...                0\n",
      "3  GN Fam going early to bed been up since or AM ...                0\n",
      "4  You think this week has been fun?!? face with ...               13\n"
     ]
    }
   ],
   "source": [
    "#Define the emoticon dictionary outside the function for a wider scope\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smile \",\n",
    "    \":(\": \"sad \",\n",
    "    \":D\": \"laugh \",\n",
    "    \"😊\": \"smiling face with smiling eyes \",\n",
    "    \"😃\": \"grinning face with big eyes \",\n",
    "    \"😉\": \"winking face \",\n",
    "    \"👌\": \"OK hand \",\n",
    "    \"👍\": \"Thumbs up \",\n",
    "    \"😁\": \"beaming face with smiling eyes \",\n",
    "    \"😂\": \"face with tears of joy \",\n",
    "    \"😄\": \"grinning face with smiling eyes \",\n",
    "    \"😅\": \"grinning face with sweat \",\n",
    "    \"😆\": \"grinning squinting face \",\n",
    "    \"😇\": \"smiling face with halo \",\n",
    "    \"😞\": \"disappointed face \",\n",
    "    \"😔\": \"pensive face \",\n",
    "    \"😑\": \"expressionless face \",\n",
    "    \"😒\": \"unamused face \",\n",
    "    \"😓\": \"downcast face with sweat \",\n",
    "    \"😕\": \"confused face \",\n",
    "    \"😖\": \"confounded face \",\n",
    "    \"💰\": \"Money Bag \",\n",
    "    \"📈\": \"Up Trend \",\n",
    "    \"🤣\": \"Rolling on the Floor Laughing \",\n",
    "    \"🎊\": \"Confetti Ball \",\n",
    "    \"😭\": \"Loudly Crying \",\n",
    "    \"🙁\": \"Slightly frowning face \",\n",
    "    \"💔\": \"Broken Heart \",\n",
    "    \"😢\": \"Crying Face \",\n",
    "    \"😮\": \"Face with Open Mouth \",\n",
    "    \"😵\": \"Dizzy Face \",\n",
    "    \"🙀\": \"Weary Cat \",\n",
    "    \"😱\": \"Face Screaming in Fear \",\n",
    "    \"❗\": \"Exclamation Mark \",\n",
    "    \"😠\": \"Angry Face \",\n",
    "    \"😡\": \"Pouting Face \",\n",
    "    \"😤\": \"Face with Steam from Nose \",\n",
    "    \"👎\": \"Thumbs Down \",\n",
    "    \"🔪\": \"Hocho \",\n",
    "    \"🌕\": \"Moon \",\n",
    "    \"🚀\": \"Rocket \",\n",
    "    \"💎\": \"Diamond \",\n",
    "    \"👀\": \"Eyes \",\n",
    "    \"💭\": \"Thought Balloon \",\n",
    "    \"📉\": \"Down Trend \",\n",
    "    \"😨\": \"Fearful Face \",\n",
    "    \"😩\": \"Weary Face \",\n",
    "    \"😰\": \"Anxious Face with Fear \",\n",
    "    \"💸\": \"Money with Wings \"\n",
    "}\n",
    "\n",
    "# Emoticon to word conversion function\n",
    "def convert_emoticons_to_words(text):\n",
    "    changed_emoticons = 0  # Variable to count the number of changed emoticons\n",
    "    for emoticon, word in emoticon_dict.items():\n",
    "        while emoticon in text:\n",
    "            text = text.replace(emoticon, word + \" \", 1)\n",
    "            changed_emoticons += 1\n",
    "    return text, changed_emoticons\n",
    "\n",
    "# Apply the function and count emoticons for each row\n",
    "def apply_conversion(text):\n",
    "    converted_text, count = convert_emoticons_to_words(text)\n",
    "    return pd.Series([converted_text, count], index=['converted_text', 'emoticons_count'])\n",
    "\n",
    "conversion_results = dataset['text'].apply(apply_conversion)\n",
    "dataset['converted_text'] = conversion_results['converted_text']\n",
    "dataset['emoticons_count'] = conversion_results['emoticons_count']\n",
    "print(\"Emoticons converted to words in 'converted_text' column.\")\n",
    "print(dataset[['converted_text', 'emoticons_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c647fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ff9d183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal applied separately after the option has been chosen and processed\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# Apply the stopwords cleaning after the loop, once the 'text' column has been updated accordingly\n",
    "dataset['text'] = dataset['converted_text'].apply(cleaning_stopwords)\n",
    "print(\"Stopwords removed from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a9069e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeating words cleaned from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to clean repeating words\n",
    "def cleaning_repeating_words(text):\n",
    "    # This regex pattern targets whole words that are repeated\n",
    "    return re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "\n",
    "# Assuming 'dataset' is a pandas DataFrame and 'text' is a column in it\n",
    "# Apply the cleaning function for repeating words to each row in the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(cleaning_repeating_words)\n",
    "print(\"Repeating words cleaned from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7e1f0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    btc on glp resistance for now play safe if u r...\n",
       "1                 hay bullfrog breakout lets fill wick\n",
       "2    did guys see pitch deck reaching community tha...\n",
       "3    gn fam going early bed since am morning nonsto...\n",
       "4    you think week fun?!? face tears joy face tear...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "92c75347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The pattern matches word characters (\\w) and punctuation marks ([^\\w\\s])\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "\n",
    "# Applying the modified tokenizer to the dataset\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f63a0715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d0802f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ffd9e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 11s 419ms/step - loss: 0.7367 - accuracy: 0.4657 - val_loss: 0.7312 - val_accuracy: 0.6429\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 0.7336 - accuracy: 0.5379 - val_loss: 0.7282 - val_accuracy: 0.6429\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.7312 - accuracy: 0.5523 - val_loss: 0.7264 - val_accuracy: 0.6286\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.7305 - accuracy: 0.5343 - val_loss: 0.7246 - val_accuracy: 0.6286\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.7257 - accuracy: 0.5632 - val_loss: 0.7232 - val_accuracy: 0.6286\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.7243 - accuracy: 0.5704 - val_loss: 0.7215 - val_accuracy: 0.6286\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 128ms/step - loss: 0.7263 - accuracy: 0.5704 - val_loss: 0.7197 - val_accuracy: 0.6286\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 131ms/step - loss: 0.7224 - accuracy: 0.5596 - val_loss: 0.7180 - val_accuracy: 0.6286\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 129ms/step - loss: 0.7222 - accuracy: 0.5632 - val_loss: 0.7162 - val_accuracy: 0.6286\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 128ms/step - loss: 0.7194 - accuracy: 0.5704 - val_loss: 0.7146 - val_accuracy: 0.6286\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 9s 483ms/step - loss: 2.2112 - accuracy: 0.2996 - val_loss: 2.2074 - val_accuracy: 0.4286\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 2.1961 - accuracy: 0.4368 - val_loss: 2.1941 - val_accuracy: 0.4857\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 132ms/step - loss: 2.1820 - accuracy: 0.5090 - val_loss: 2.1806 - val_accuracy: 0.4857\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 2.1658 - accuracy: 0.5379 - val_loss: 2.1663 - val_accuracy: 0.4857\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 151ms/step - loss: 2.1483 - accuracy: 0.5343 - val_loss: 2.1507 - val_accuracy: 0.4857\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 2.1307 - accuracy: 0.5415 - val_loss: 2.1334 - val_accuracy: 0.4857\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 2.1079 - accuracy: 0.5451 - val_loss: 2.1139 - val_accuracy: 0.4857\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 2.0815 - accuracy: 0.5451 - val_loss: 2.0919 - val_accuracy: 0.4857\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 2.0524 - accuracy: 0.5451 - val_loss: 2.0656 - val_accuracy: 0.4857\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 2.0264 - accuracy: 0.5451 - val_loss: 2.0351 - val_accuracy: 0.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize the tokenizer with your dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "\n",
    "# Function to create LSTM model, now with optimizer initialization inside\n",
    "def create_lstm_model(input_length, num_classes):\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))\n",
    "    # Adding Bidirectional LSTM and regularization\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    # Added L2 regularization to the Dense layer\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare the dataset for training\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder_polarity = LabelEncoder()\n",
    "y_polarity = to_categorical(encoder_polarity.fit_transform(dataset['polarity']))\n",
    "\n",
    "encoder_emotion = LabelEncoder()\n",
    "y_emotion = to_categorical(encoder_emotion.fit_transform(dataset['emotion']))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_polarity, X_test_polarity, y_train_polarity, y_test_polarity = train_test_split(X, y_polarity, test_size=0.2, random_state=42)\n",
    "X_train_emotion, X_test_emotion, y_train_emotion, y_test_emotion = train_test_split(X, y_emotion, test_size=0.2, random_state=42)\n",
    "\n",
    "# When creating the models, we do not pass the optimizer as a parameter anymore\n",
    "lstm_model_polarity = create_lstm_model(50, y_polarity.shape[1])\n",
    "lstm_model_emotion = create_lstm_model(50, y_emotion.shape[1])\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the LSTM models with EarlyStopping and validation data\n",
    "lstm_model_polarity.fit(\n",
    "    X_train_polarity, y_train_polarity, \n",
    "    epochs=10, \n",
    "    batch_size=64,  # Ensure batch size divides the number of samples evenly or set shuffle=True\n",
    "    validation_data=(X_test_polarity, y_test_polarity),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "lstm_model_emotion.fit(\n",
    "    X_train_emotion, y_train_emotion, \n",
    "    epochs=10, \n",
    "    batch_size=64,  # Same batch size for consistency and ease of computation\n",
    "    validation_data=(X_test_emotion, y_test_emotion),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained LSTM models\n",
    "lstm_model_polarity.save('lstm_polarity_model.h5')\n",
    "lstm_model_emotion.save('lstm_emotion_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dfcb89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 28ms/step\n",
      "9/9 [==============================] - 2s 27ms/step\n",
      "3/3 [==============================] - 1s 12ms/step\n",
      "3/3 [==============================] - 1s 17ms/step\n",
      "Polarity Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.42      0.41        26\n",
      "           1       0.64      0.61      0.63        44\n",
      "\n",
      "    accuracy                           0.54        70\n",
      "   macro avg       0.52      0.52      0.52        70\n",
      "weighted avg       0.55      0.54      0.55        70\n",
      "\n",
      "Emotion Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.44      0.35      0.39        23\n",
      "           2       0.20      0.29      0.24         7\n",
      "           3       0.48      0.47      0.48        34\n",
      "           5       0.00      0.00      0.00         3\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.37        70\n",
      "   macro avg       0.19      0.18      0.18        70\n",
      "weighted avg       0.40      0.37      0.38        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features(model, data):\n",
    "    # Create a model that will return the outputs of the LSTM layer\n",
    "    # Adjust the layer index (-2) based on where your LSTM layer is located within the model\n",
    "    intermediate_layer_model = Model(inputs=model.input, \n",
    "                                     outputs=model.layers[-2].output)\n",
    "    # Get the features from the data using the intermediate model\n",
    "    features = intermediate_layer_model.predict(data)\n",
    "    return features\n",
    "\n",
    "# Extract features with LSTM for SVM training\n",
    "X_train_features_polarity = extract_features(lstm_model_polarity, X_train_polarity)\n",
    "X_train_features_emotion = extract_features(lstm_model_emotion, X_train_emotion)\n",
    "\n",
    "# Normalize the features\n",
    "scaler_polarity = StandardScaler().fit(X_train_features_polarity)\n",
    "X_train_features_polarity = scaler_polarity.transform(X_train_features_polarity)\n",
    "\n",
    "scaler_emotion = StandardScaler().fit(X_train_features_emotion)\n",
    "X_train_features_emotion = scaler_emotion.transform(X_train_features_emotion)\n",
    "\n",
    "# Train SVM for polarity and emotion\n",
    "# Consider using GridSearchCV for hyperparameter tuning here\n",
    "svm_classifier_polarity = SVC(kernel='linear', probability=True)\n",
    "svm_classifier_emotion = SVC(kernel='linear', probability=True)\n",
    "\n",
    "svm_classifier_polarity.fit(X_train_features_polarity, np.argmax(y_train_polarity, axis=1))\n",
    "svm_classifier_emotion.fit(X_train_features_emotion, np.argmax(y_train_emotion, axis=1))\n",
    "\n",
    "# Evaluate SVM on test set\n",
    "X_test_features_polarity = scaler_polarity.transform(extract_features(lstm_model_polarity, X_test_polarity))\n",
    "X_test_features_emotion = scaler_emotion.transform(extract_features(lstm_model_emotion, X_test_emotion))\n",
    "\n",
    "y_pred_polarity = svm_classifier_polarity.predict(X_test_features_polarity)\n",
    "y_pred_emotion = svm_classifier_emotion.predict(X_test_features_emotion)\n",
    "\n",
    "print(\"Polarity Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test_polarity, axis=1), y_pred_polarity))\n",
    "\n",
    "print(\"Emotion Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test_emotion, axis=1), y_pred_emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "45cf3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_emotion_model.joblib']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained SVM models using joblib or pickle\n",
    "import joblib\n",
    "joblib.dump(svm_classifier_polarity, 'svm_polarity_model.joblib')\n",
    "joblib.dump(svm_classifier_emotion, 'svm_emotion_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "843cc28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 13ms/step\n",
      "Polarity Precision: 0.3951\n",
      "Polarity Recall: 0.6286\n",
      "Polarity F1 Score: 0.4852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set for polarity\n",
    "y_pred_polarity = lstm_model_polarity.predict(X_test_polarity)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_polarity = np.argmax(y_pred_polarity, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_polarity = np.argmax(y_test_polarity, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for polarity\n",
    "precision_polarity = precision_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "recall_polarity = recall_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "f1_score_polarity = f1_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "\n",
    "print(f'Polarity Precision: {precision_polarity:.4f}')\n",
    "print(f'Polarity Recall: {recall_polarity:.4f}')\n",
    "print(f'Polarity F1 Score: {f1_score_polarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b4e3a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 15ms/step\n",
      "Emotion Precision: 0.2359\n",
      "Emotion Recall: 0.4857\n",
      "Emotion F1 Score: 0.3176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set for emotion\n",
    "y_pred_emotion = lstm_model_emotion.predict(X_test_emotion)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_emotion = np.argmax(y_pred_emotion, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_emotion = np.argmax(y_test_emotion, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for emotion\n",
    "precision_emotion = precision_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "recall_emotion = recall_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "f1_score_emotion = f1_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "\n",
    "print(f'Emotion Precision: {precision_emotion:.4f}')\n",
    "print(f'Emotion Recall: {recall_emotion:.4f}')\n",
    "print(f'Emotion F1 Score: {f1_score_emotion:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ba9a6d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Tweet: I'm angry\n",
      "Polarity Label: neutral\n",
      "Emotion Label: happy\n",
      "Intensity Level: Low\n"
     ]
    }
   ],
   "source": [
    "# Assuming `polarity_labels` is your list of original labels for the training data\n",
    "# Example list of polarity labels used in your training dataset\n",
    "polarity_labels = ['positive', 'negative', 'neutral']  # This should be replaced by the actual labels you have\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder_polarity = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder with your actual labels\n",
    "encoder_polarity.fit(polarity_labels)\n",
    "\n",
    "# Assuming encoder is a pre-defined LabelEncoder object for decoding the emotion labels\n",
    "\n",
    "# This function should extract LSTM features\n",
    "def extract_features(model, sequence):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    return intermediate_layer_model.predict(sequence)\n",
    "\n",
    "# Function to classify the intensity\n",
    "def classify_intensity(emoticons_count, text):\n",
    "    question_marks = text.count('?')\n",
    "    periods = text.count('.')\n",
    "    exclamation_marks = text.count('!')\n",
    "\n",
    "    if exclamation_marks > 1 or question_marks > 1 or emoticons_count > 1:\n",
    "        return 'High'\n",
    "    elif periods == 1 or question_marks == 1 or emoticons_count == 1 or exclamation_marks ==1 :\n",
    "        return 'Medium'\n",
    "    elif question_marks == 0 and emoticons_count == 0:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Undetermined'\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "    return ' '.join(tokenizer.tokenize(text))\n",
    "\n",
    "#Function to perform real-time prediction and intensity classification\n",
    "def real_time_prediction(text, tokenizer, lstm_model_emotion, lstm_model_polarity, svm_classifier_emotion, svm_classifier_polarity, encoder_emotion, encoder_polarity):\n",
    "    # Preprocessing steps (assuming these functions are defined elsewhere in your code)\n",
    "\n",
    "\n",
    "    cleaned_text = cleaning_numbers(text)\n",
    "    cleaned_tweet = clean_tweet(cleaned_text)\n",
    "    corrected_text = spell_correction(cleaned_tweet)\n",
    "    emoticon_converted_text, emoticons_count = convert_emoticons_to_words(corrected_text)  # Ensure this function returns emoticons_count\n",
    "    cleaned_stopwords = cleaning_stopwords(emoticon_converted_text)\n",
    "    cleaned_repeating_words = cleaning_repeating_words(cleaned_stopwords)\n",
    "\n",
    "    # Now tokenize the text after cleaning repeating words\n",
    "    tokenized_text = tokenize_text(cleaned_repeating_words)\n",
    "\n",
    "    # Continue with any additional preprocessing steps that work on the tokenized text\n",
    "    stemmed_text = stemming_on_text(tokenized_text)\n",
    "    lemmatized_text = lemmatizer_on_text(stemmed_text)\n",
    "\n",
    "    assert isinstance(lemmatized_text, str), \"Processed text must be a string\"\n",
    "\n",
    "    # Convert the processed text to a sequence\n",
    "    sequence = tokenizer.texts_to_sequences([lemmatized_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=50)\n",
    "\n",
    "\n",
    "    # Predict emotion and polarity using the LSTM model\n",
    "    lstm_prediction_emotion = lstm_model_emotion.predict(padded_sequence)\n",
    "    lstm_features_emotion = extract_features(lstm_model_emotion, padded_sequence)\n",
    "    svm_prediction_emotion = svm_classifier_emotion.predict_proba(lstm_features_emotion)\n",
    "\n",
    "    lstm_prediction_polarity = lstm_model_polarity.predict(padded_sequence)\n",
    "    lstm_features_polarity = extract_features(lstm_model_polarity, padded_sequence)\n",
    "    svm_prediction_polarity = svm_classifier_polarity.predict_proba(lstm_features_polarity)\n",
    "\n",
    "    # Decode the predicted labels\n",
    "    emotion_label = encoder_emotion.inverse_transform(np.argmax(lstm_prediction_emotion, axis=1))\n",
    "    polarity_label = encoder_polarity.inverse_transform(np.argmax(svm_prediction_polarity, axis=1))\n",
    "\n",
    "    # Get probabilities for the predicted labels\n",
    "    emotion_probability = np.max(lstm_prediction_emotion, axis=1)\n",
    "    polarity_probability = np.max(svm_prediction_polarity, axis=1)\n",
    "\n",
    "    # Classify the intensity\n",
    "    intensity = classify_intensity(emoticons_count, text)  # Ensure `emoticons_count` is defined\n",
    "\n",
    "    return polarity_label, emotion_label, polarity_probability, emotion_probability, intensity\n",
    "\n",
    "#This is the real time tweets\n",
    "tweet = \"I'm angry\"    \n",
    "# Call the real-time prediction function\n",
    "polarity_label, emotion_label, polarity_probability, emotion_probability, intensity = real_time_prediction(tweet, tokenizer, lstm_model_emotion, lstm_model_polarity, svm_classifier_emotion, svm_classifier_polarity, encoder_emotion, encoder_polarity)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Tweet: {tweet}\")\n",
    "print(f\"Polarity Label: {polarity_label[0]}\")\n",
    "print(f\"Emotion Label: {emotion_label[0]}\")\n",
    "print(f\"Intensity Level: {intensity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

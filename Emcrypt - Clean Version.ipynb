{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a48e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#SpellCorrection\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import string\n",
    "import emoji\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ec4dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: UTF-8-SIG\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>10/28/23</td>\n",
       "      <td>@ja1405_ja</td>\n",
       "      <td>#Btc¬† back to ATH is only 2X, this can happen ...</td>\n",
       "      <td>2</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>10/30/23</td>\n",
       "      <td>@CryptoChef_M</td>\n",
       "      <td>#Bitcoin Weekly Forecastüìå‚ÄºÔ∏è Oct. 30 - Nov.05 K...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>10/27/23</td>\n",
       "      <td>@Rao_Cash</td>\n",
       "      <td>Rao Cash is the new bitcoin killer, earn X100,...</td>\n",
       "      <td>2</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>10/27/23</td>\n",
       "      <td>@VipinvestmentsI</td>\n",
       "      <td>The future of digital asset management is here...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>8:05 PM ¬∑ Oct 23, 2023</td>\n",
       "      <td>@Eldorado_krypto</td>\n",
       "      <td>üìù The ruling urges banks not to freeze account...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date          username  \\\n",
       "407                10/28/23        @ja1405_ja   \n",
       "563                10/30/23     @CryptoChef_M   \n",
       "511                10/27/23         @Rao_Cash   \n",
       "508                10/27/23  @VipinvestmentsI   \n",
       "127  8:05 PM ¬∑ Oct 23, 2023  @Eldorado_krypto   \n",
       "\n",
       "                                                  text  polarity       emotion  \n",
       "407  #Btc¬† back to ATH is only 2X, this can happen ...         2  anticipation  \n",
       "563  #Bitcoin Weekly Forecastüìå‚ÄºÔ∏è Oct. 30 - Nov.05 K...         2         happy  \n",
       "511  Rao Cash is the new bitcoin killer, earn X100,...         2     surprised  \n",
       "508  The future of digital asset management is here...         2         happy  \n",
       "127  üìù The ruling urges banks not to freeze account...         2         happy  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['date', 'username', 'text', 'polarity', 'emotion']\n",
    "\n",
    "#Detect file encoding using chardet\n",
    "with open('data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Print the detected encoding\n",
    "print(\"Detected encoding:\", result['encoding'])\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "df = pd.read_csv('data.csv', encoding=result['encoding'], names=DATASET_COLUMNS)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10f5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "data=df[['text','polarity', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed21aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ba51c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['polarity'] == 2]\n",
    "data_neu = data[data['polarity'] == 1]\n",
    "data_neg = data[data['polarity'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6d97b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(100)]\n",
    "data_neu = data_neu.iloc[:int(100)]\n",
    "data_neg = data_neg.iloc[:int(100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0888a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([data_pos, data_neu, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8bd68dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450    Hey Websec fam, We're collaborating with @jobs...\n",
       "452    Shill me your next Big #Crypto ready to Explod...\n",
       "457    The eyes of the Geishas...watching you. Why ar...\n",
       "461    What is the concept that a blockchain cannot a...\n",
       "462    Web innovation: endless opportunities. #crypto...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3469fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1            HAY bullflag breakoutüëÄ Lets fill that wicküöÄ\n",
      "2      Did you guys see how is doing a pitch with a d...\n",
      "3      GN Fam going early to bed been up since or AM ...\n",
      "4      You think this week has been fun?!? üòÇüòÇüòÇüòÇ Wait ...\n",
      "                             ...                        \n",
      "450    Hey Websec fam Were collaborating with Get top...\n",
      "452            Shill me your next Big ready to ExplodeüöÄüìà\n",
      "457    The eyes of the Geishas...watching you. Why ar...\n",
      "461    What is the concept that a blockchain cannot a...\n",
      "462                Web innovation endless opportunities.\n",
      "Name: text, Length: 200, dtype: object\n"
     ]
    }
   ],
   "source": [
    "emoticons_to_keep = [\n",
    "    'üí∞', 'üìà', 'ü§£', 'üéä', 'üòÇ', 'üò≠', 'üôÅ', 'üòû', 'üíî', 'üò¢', 'üòÆ', 'üòµ', 'üôÄ',\n",
    "    'üò±', '‚ùó', 'üò†', 'üò°', 'üò§', 'üëé', 'üî™', 'üåï', 'üöÄ', 'üíé', 'üëÄ', 'üí≠', 'üìâ',\n",
    "    'üò®', 'üò©', 'üò∞', 'üí∏'\n",
    "]\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove special characters except for emoticons\n",
    "    text = re.sub(r'[^\\w\\s.!?{}]+'.format(''.join(emoticons_to_keep)), '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the modified cleaning function to the 'text' column in your dataset\n",
    "dataset['text'] = dataset['text'].apply(clean_tweet)\n",
    "\n",
    "# Display the 'text' column in the entire dataset\n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9db4387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  polarity       emotion\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...         2         happy\n",
      "1            HAY bullfrog breakout Lets fill that wick         2  anticipation\n",
      "2    Did you guys see how is doing a pitch with a d...         2         happy\n",
      "3    GN Fam going early to bed been up since or AM ...         2         happy\n",
      "4    You think this week has been fun?!? üòÇüòÇüòÇüòÇ Wait ...         2  anticipation\n",
      "..                                                 ...       ...           ...\n",
      "450  Hey Websec fam Were collaborating with Get top...         1       Neutral\n",
      "452          Shill me your next Big ready to ExplodeüöÄüìà         1       Neutral\n",
      "457  The eyes of the Geishas...watching you Why are...         1       Neutral\n",
      "461  What is the concept that a blockchain cannot a...         1       Neutral\n",
      "462               Web innovation endless opportunities         1       Neutral\n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SpellChecker only once to avoid re-creation for each call\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function for spell correction\n",
    "def spell_correction(text):\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Check if the correction is not None, otherwise use the original word\n",
    "            corrected_words.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply spell correction to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(spell_correction)\n",
    "\n",
    "# Display the entire dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52520eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons converted to words in 'converted_text' column.\n",
      "                                      converted_text  emoticons_count\n",
      "0  BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...                0\n",
      "1          HAY bullfrog breakout Lets fill that wick                0\n",
      "2  Did you guys see how is doing a pitch with a d...                0\n",
      "3  GN Fam going early to bed been up since or AM ...                0\n",
      "4  You think this week has been fun?!? face with ...               13\n"
     ]
    }
   ],
   "source": [
    "#Define the emoticon dictionary outside the function for a wider scope\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smile \",\n",
    "    \":(\": \"sad \",\n",
    "    \":D\": \"laugh \",\n",
    "    \"üòä\": \"smiling face with smiling eyes \",\n",
    "    \"üòÉ\": \"grinning face with big eyes \",\n",
    "    \"üòâ\": \"winking face \",\n",
    "    \"üëå\": \"OK hand \",\n",
    "    \"üëç\": \"Thumbs up \",\n",
    "    \"üòÅ\": \"beaming face with smiling eyes \",\n",
    "    \"üòÇ\": \"face with tears of joy \",\n",
    "    \"üòÑ\": \"grinning face with smiling eyes \",\n",
    "    \"üòÖ\": \"grinning face with sweat \",\n",
    "    \"üòÜ\": \"grinning squinting face \",\n",
    "    \"üòá\": \"smiling face with halo \",\n",
    "    \"üòû\": \"disappointed face \",\n",
    "    \"üòî\": \"pensive face \",\n",
    "    \"üòë\": \"expressionless face \",\n",
    "    \"üòí\": \"unamused face \",\n",
    "    \"üòì\": \"downcast face with sweat \",\n",
    "    \"üòï\": \"confused face \",\n",
    "    \"üòñ\": \"confounded face \",\n",
    "    \"üí∞\": \"Money Bag \",\n",
    "    \"üìà\": \"Up Trend \",\n",
    "    \"ü§£\": \"Rolling on the Floor Laughing \",\n",
    "    \"üéä\": \"Confetti Ball \",\n",
    "    \"üò≠\": \"Loudly Crying \",\n",
    "    \"üôÅ\": \"Slightly frowning face \",\n",
    "    \"üíî\": \"Broken Heart \",\n",
    "    \"üò¢\": \"Crying Face \",\n",
    "    \"üòÆ\": \"Face with Open Mouth \",\n",
    "    \"üòµ\": \"Dizzy Face \",\n",
    "    \"üôÄ\": \"Weary Cat \",\n",
    "    \"üò±\": \"Face Screaming in Fear \",\n",
    "    \"‚ùó\": \"Exclamation Mark \",\n",
    "    \"üò†\": \"Angry Face \",\n",
    "    \"üò°\": \"Pouting Face \",\n",
    "    \"üò§\": \"Face with Steam from Nose \",\n",
    "    \"üëé\": \"Thumbs Down \",\n",
    "    \"üî™\": \"Hocho \",\n",
    "    \"üåï\": \"Moon \",\n",
    "    \"üöÄ\": \"Rocket \",\n",
    "    \"üíé\": \"Diamond \",\n",
    "    \"üëÄ\": \"Eyes \",\n",
    "    \"üí≠\": \"Thought Balloon \",\n",
    "    \"üìâ\": \"Down Trend \",\n",
    "    \"üò®\": \"Fearful Face \",\n",
    "    \"üò©\": \"Weary Face \",\n",
    "    \"üò∞\": \"Anxious Face with Fear \",\n",
    "    \"üí∏\": \"Money with Wings \"\n",
    "}\n",
    "\n",
    "# Emoticon to word conversion function\n",
    "def convert_emoticons_to_words(text):\n",
    "    changed_emoticons = 0  # Variable to count the number of changed emoticons\n",
    "    for emoticon, word in emoticon_dict.items():\n",
    "        while emoticon in text:\n",
    "            text = text.replace(emoticon, word + \" \", 1)\n",
    "            changed_emoticons += 1\n",
    "    return text, changed_emoticons\n",
    "\n",
    "# Apply the function and count emoticons for each row\n",
    "def apply_conversion(text):\n",
    "    converted_text, count = convert_emoticons_to_words(text)\n",
    "    return pd.Series([converted_text, count], index=['converted_text', 'emoticons_count'])\n",
    "\n",
    "conversion_results = dataset['text'].apply(apply_conversion)\n",
    "dataset['converted_text'] = conversion_results['converted_text']\n",
    "dataset['emoticons_count'] = conversion_results['emoticons_count']\n",
    "print(\"Emoticons converted to words in 'converted_text' column.\")\n",
    "print(dataset[['converted_text', 'emoticons_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c647fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff9d183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal applied separately after the option has been chosen and processed\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# Apply the stopwords cleaning after the loop, once the 'text' column has been updated accordingly\n",
    "dataset['text'] = dataset['converted_text'].apply(cleaning_stopwords)\n",
    "print(\"Stopwords removed from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9069e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeating words cleaned from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to clean repeating words\n",
    "def cleaning_repeating_words(text):\n",
    "    # This regex pattern targets whole words that are repeated\n",
    "    return re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "\n",
    "# Assuming 'dataset' is a pandas DataFrame and 'text' is a column in it\n",
    "# Apply the cleaning function for repeating words to each row in the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(cleaning_repeating_words)\n",
    "print(\"Repeating words cleaned from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e1f0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    btc on glp resistance for now play safe if u r...\n",
       "1                 hay bullfrog breakout lets fill wick\n",
       "2    did guys see pitch deck reaching community tha...\n",
       "3    gn fam going early bed since am morning nonsto...\n",
       "4    you think week fun?!? face tears joy face tear...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92c75347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The pattern matches word characters (\\w) and punctuation marks ([^\\w\\s])\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "\n",
    "# Applying the modified tokenizer to the dataset\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f63a0715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0802f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffd9e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 4s 418ms/step - loss: 0.6918 - accuracy: 0.5562 - val_loss: 0.6923 - val_accuracy: 0.4750\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.6915 - accuracy: 0.5437 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.6904 - accuracy: 0.5688 - val_loss: 0.6912 - val_accuracy: 0.5250\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.6888 - accuracy: 0.6125 - val_loss: 0.6907 - val_accuracy: 0.5500\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 86ms/step - loss: 0.6885 - accuracy: 0.6250 - val_loss: 0.6902 - val_accuracy: 0.6000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.6869 - accuracy: 0.6500 - val_loss: 0.6897 - val_accuracy: 0.6750\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.6845 - accuracy: 0.7688 - val_loss: 0.6891 - val_accuracy: 0.6500\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.6844 - accuracy: 0.7563 - val_loss: 0.6886 - val_accuracy: 0.6500\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.6824 - accuracy: 0.7812 - val_loss: 0.6881 - val_accuracy: 0.6500\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.6817 - accuracy: 0.7750 - val_loss: 0.6875 - val_accuracy: 0.6250\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 5s 920ms/step - loss: 2.0771 - accuracy: 0.1250 - val_loss: 2.0741 - val_accuracy: 0.1500\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 133ms/step - loss: 2.0758 - accuracy: 0.1625 - val_loss: 2.0728 - val_accuracy: 0.2000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 2.0758 - accuracy: 0.1250 - val_loss: 2.0715 - val_accuracy: 0.2500\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 128ms/step - loss: 2.0719 - accuracy: 0.1688 - val_loss: 2.0702 - val_accuracy: 0.2750\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 2.0698 - accuracy: 0.2500 - val_loss: 2.0688 - val_accuracy: 0.3250\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 141ms/step - loss: 2.0688 - accuracy: 0.1937 - val_loss: 2.0675 - val_accuracy: 0.3750\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 142ms/step - loss: 2.0647 - accuracy: 0.2937 - val_loss: 2.0661 - val_accuracy: 0.4250\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 2.0630 - accuracy: 0.3375 - val_loss: 2.0647 - val_accuracy: 0.4250\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 172ms/step - loss: 2.0606 - accuracy: 0.3500 - val_loss: 2.0633 - val_accuracy: 0.4250\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 2.0573 - accuracy: 0.3500 - val_loss: 2.0618 - val_accuracy: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# All necessary imports should be placed at the top of the script\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Ensure `dataset` is correctly loaded with 'text' column\n",
    "# dataset = pd.read_csv('your_dataset.csv') # Uncomment and set your dataset path\n",
    "# Make sure 'text', 'polarity', and 'emotion' columns exist\n",
    "# print(dataset.columns) # Uncomment to check columns\n",
    "\n",
    "# Initialize the tokenizer with your dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "\n",
    "# Function to create LSTM model, now with optimizer initialization inside\n",
    "def create_lstm_model(input_length, num_classes):\n",
    "    # Initialize the optimizer with the desired settings inside the function\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # The optimizer is used here directly after its initialization\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Adjust the learning rate\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Prepare the dataset for training\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder_polarity = LabelEncoder()\n",
    "y_polarity = to_categorical(encoder_polarity.fit_transform(dataset['polarity']))\n",
    "\n",
    "encoder_emotion = LabelEncoder()\n",
    "y_emotion = to_categorical(encoder_emotion.fit_transform(dataset['emotion']))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_polarity, X_test_polarity, y_train_polarity, y_test_polarity = train_test_split(X, y_polarity, test_size=0.2, random_state=42)\n",
    "X_train_emotion, X_test_emotion, y_train_emotion, y_test_emotion = train_test_split(X, y_emotion, test_size=0.2, random_state=42)\n",
    "\n",
    "# When creating the models, we do not pass the optimizer as a parameter anymore\n",
    "lstm_model_polarity = create_lstm_model(50, y_polarity.shape[1])\n",
    "lstm_model_emotion = create_lstm_model(50, y_emotion.shape[1])\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the LSTM models with EarlyStopping and validation data\n",
    "lstm_model_polarity.fit(\n",
    "    X_train_polarity, y_train_polarity, \n",
    "    epochs=10, \n",
    "    batch_size=64,  # Ensure batch size divides the number of samples evenly or set shuffle=True\n",
    "    validation_data=(X_test_polarity, y_test_polarity),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "lstm_model_emotion.fit(\n",
    "    X_train_emotion, y_train_emotion, \n",
    "    epochs=10, \n",
    "    batch_size=64,  # Same batch size for consistency and ease of computation\n",
    "    validation_data=(X_test_emotion, y_test_emotion),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained LSTM models\n",
    "lstm_model_polarity.save('lstm_polarity_model.h5')\n",
    "lstm_model_emotion.save('lstm_emotion_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfcb89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbebe0740d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5/5 [==============================] - 1s 10ms/step\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbea8c89160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "5/5 [==============================] - 1s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def extract_features(model, data):\n",
    "    # Create a model that will return the outputs of the LSTM layer\n",
    "    # Adjust the layer index (-2) based on where your LSTM layer is located within the model\n",
    "    intermediate_layer_model = Model(inputs=model.input, \n",
    "                                     outputs=model.layers[-2].output)\n",
    "    # Get the features from the data using the intermediate model\n",
    "    features = intermediate_layer_model.predict(data)\n",
    "    return features\n",
    "\n",
    "# Extract features with LSTM for SVM training\n",
    "# Assuming lstm_model_polarity and lstm_model_emotion are already defined and trained LSTM models\n",
    "X_train_features_polarity = extract_features(lstm_model_polarity, X_train_polarity)\n",
    "X_train_features_emotion = extract_features(lstm_model_emotion, X_train_emotion)\n",
    "\n",
    "\n",
    "# Train SVM for polarity and emotion\n",
    "svm_classifier_polarity = SVC(kernel='linear', probability=True)\n",
    "svm_classifier_emotion = SVC(kernel='linear', probability=True)\n",
    "\n",
    "svm_classifier_polarity.fit(X_train_features_polarity, np.argmax(y_train_polarity, axis=1))\n",
    "svm_classifier_emotion.fit(X_train_features_emotion, np.argmax(y_train_emotion, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45cf3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_emotion_model.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained SVM models using joblib or pickle\n",
    "import joblib\n",
    "joblib.dump(svm_classifier_polarity, 'svm_polarity_model.joblib')\n",
    "joblib.dump(svm_classifier_emotion, 'svm_emotion_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "843cc28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 10ms/step\n",
      "Polarity Precision: 0.6262\n",
      "Polarity Recall: 0.6250\n",
      "Polarity F1 Score: 0.6252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set for polarity\n",
    "y_pred_polarity = lstm_model_polarity.predict(X_test_polarity)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_polarity = np.argmax(y_pred_polarity, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_polarity = np.argmax(y_test_polarity, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for polarity\n",
    "precision_polarity = precision_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "recall_polarity = recall_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "f1_score_polarity = f1_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "\n",
    "print(f'Polarity Precision: {precision_polarity:.4f}')\n",
    "print(f'Polarity Recall: {recall_polarity:.4f}')\n",
    "print(f'Polarity F1 Score: {f1_score_polarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4e3a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 8ms/step\n",
      "Emotion Precision: 0.4598\n",
      "Emotion Recall: 0.4500\n",
      "Emotion F1 Score: 0.3835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set for emotion\n",
    "y_pred_emotion = lstm_model_emotion.predict(X_test_emotion)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_emotion = np.argmax(y_pred_emotion, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_emotion = np.argmax(y_test_emotion, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for emotion\n",
    "precision_emotion = precision_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "recall_emotion = recall_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "f1_score_emotion = f1_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "\n",
    "print(f'Emotion Precision: {precision_emotion:.4f}')\n",
    "print(f'Emotion Recall: {recall_emotion:.4f}')\n",
    "print(f'Emotion F1 Score: {f1_score_emotion:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2a48e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#SpellCorrection\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import string\n",
    "import emoji\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7ec4dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: UTF-8-SIG\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10:05 AM · Oct 25, 2023</td>\n",
       "      <td>@cryptounitrade</td>\n",
       "      <td>✅ #PUMP $TOMO! +0.796% in 15 seconds\\nCurrent ...</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>6:25 PM · Oct 25, 2023</td>\n",
       "      <td>@CryptoScamNever</td>\n",
       "      <td>\"🚨 Attention all crypto enthusiasts! Stay one ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>12:33 AM · Oct 26, 2023</td>\n",
       "      <td>\\n@yevano__oruvan</td>\n",
       "      <td>Here are my drawings! Let's take a look and le...</td>\n",
       "      <td>1</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>7:31 AM · Oct 24, 2023</td>\n",
       "      <td>@CallBotCrypto</td>\n",
       "      <td>How could AI foresee this? #BTC\\n\\n📊📈Inside th...</td>\n",
       "      <td>1</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>12:01 PM · Oct 25, 2023\\n</td>\n",
       "      <td>@Jyo5200</td>\n",
       "      <td>📈 Just found this old screenshot of #Bitcoin  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          date           username  \\\n",
       "31     10:05 AM · Oct 25, 2023    @cryptounitrade   \n",
       "241     6:25 PM · Oct 25, 2023   @CryptoScamNever   \n",
       "179    12:33 AM · Oct 26, 2023  \\n@yevano__oruvan   \n",
       "57      7:31 AM · Oct 24, 2023     @CallBotCrypto   \n",
       "292  12:01 PM · Oct 25, 2023\\n           @Jyo5200   \n",
       "\n",
       "                                                  text  polarity   emotion  \n",
       "31   ✅ #PUMP $TOMO! +0.796% in 15 seconds\\nCurrent ...         1     happy  \n",
       "241  \"🚨 Attention all crypto enthusiasts! Stay one ...        -1      fear  \n",
       "179  Here are my drawings! Let's take a look and le...         1     happy  \n",
       "57   How could AI foresee this? #BTC\\n\\n📊📈Inside th...         1  surprise  \n",
       "292  📈 Just found this old screenshot of #Bitcoin  ...         1  surprise  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['date', 'username', 'text', 'polarity', 'emotion']\n",
    "\n",
    "#Detect file encoding using chardet\n",
    "with open('data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Print the detected encoding\n",
    "print(\"Detected encoding:\", result['encoding'])\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "df = pd.read_csv('data.csv', encoding=result['encoding'], names=DATASET_COLUMNS)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "10f5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "data=df[['text','polarity', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ed21aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "2ba51c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['polarity'] == 1]\n",
    "data_neu = data[data['polarity'] == 0]\n",
    "data_neg = data[data['polarity'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b6d97b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(100)]\n",
    "data_neu = data_neu.iloc[:int(100)]\n",
    "data_neg = data_neg.iloc[:int(100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0888a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([data_pos, data_neu, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b8bd68dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247    #alts , after nailing the whole of it .i think...\n",
       "261    Be cautious with unofficial apps and websites ...\n",
       "277    Cryptocurrency lending firm BlockFi has declar...\n",
       "285    Ffs #cryptocurrency stop give me - more hours ...\n",
       "287    Bad News for the #crypto market.\\n\\n📌The ticke...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3469fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1            HAY bullflag breakout👀 Lets fill that wick🚀\n",
      "2      Did you guys see how is doing a pitch with a d...\n",
      "3      GN Fam going early to bed been up since or AM ...\n",
      "4      You think this week has been fun?!? 😂😂😂😂 Wait ...\n",
      "                             ...                        \n",
      "247    after nailing the whole of it .i think its tim...\n",
      "261    Be cautious with unofficial apps and websites ...\n",
      "277    Cryptocurrency lending firm BlockFi has declar...\n",
      "285    Ffs stop give me more hours to accumulate jesu...\n",
      "287    Bad News for the market. The ticker for BlackR...\n",
      "Name: text, Length: 147, dtype: object\n"
     ]
    }
   ],
   "source": [
    "emoticons_to_keep = [\n",
    "    '💰', '📈', '🤣', '🎊', '😂', '😭', '🙁', '😞', '💔', '😢', '😮', '😵', '🙀',\n",
    "    '😱', '❗', '😠', '😡', '😤', '👎', '🔪', '🌕', '🚀', '💎', '👀', '💭', '📉',\n",
    "    '😨', '😩', '😰', '💸'\n",
    "]\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove special characters except for emoticons\n",
    "    text = re.sub(r'[^\\w\\s.!?{}]+'.format(''.join(emoticons_to_keep)), '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the modified cleaning function to the 'text' column in your dataset\n",
    "dataset['text'] = dataset['text'].apply(clean_tweet)\n",
    "\n",
    "# Display the 'text' column in the entire dataset\n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9db4387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  polarity       emotion\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...         1         happy\n",
      "1            HAY bullfrog breakout Lets fill that wick         1  anticipation\n",
      "2    Did you guys see how is doing a pitch with a d...         1         happy\n",
      "3    GN Fam going early to bed been up since or AM ...         1         happy\n",
      "4    You think this week has been fun?!? 😂😂😂😂 Wait ...         1  anticipation\n",
      "..                                                 ...       ...           ...\n",
      "247  after nailing the whole of it i think its time...        -1          fear\n",
      "261  Be cautious with unofficial apps and websites ...        -1          fear\n",
      "277  Cryptocurrency lending firm BlockFi has declar...        -1           sad\n",
      "285  Ffs stop give me more hours to accumulate jesu...        -1         angry\n",
      "287  Bad News for the market The ticker for BlackRo...        -1           sad\n",
      "\n",
      "[147 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SpellChecker only once to avoid re-creation for each call\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function for spell correction\n",
    "def spell_correction(text):\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Check if the correction is not None, otherwise use the original word\n",
    "            corrected_words.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply spell correction to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(spell_correction)\n",
    "\n",
    "# Display the entire dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "52520eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons converted to words in 'converted_text' column.\n",
      "                                      converted_text  emoticons_count\n",
      "0  BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...                0\n",
      "1          HAY bullfrog breakout Lets fill that wick                0\n",
      "2  Did you guys see how is doing a pitch with a d...                0\n",
      "3  GN Fam going early to bed been up since or AM ...                0\n",
      "4  You think this week has been fun?!? face with ...               13\n"
     ]
    }
   ],
   "source": [
    "#Define the emoticon dictionary outside the function for a wider scope\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smile \",\n",
    "    \":(\": \"sad \",\n",
    "    \":D\": \"laugh \",\n",
    "    \"😊\": \"smiling face with smiling eyes \",\n",
    "    \"😃\": \"grinning face with big eyes \",\n",
    "    \"😉\": \"winking face \",\n",
    "    \"👌\": \"OK hand \",\n",
    "    \"👍\": \"Thumbs up \",\n",
    "    \"😁\": \"beaming face with smiling eyes \",\n",
    "    \"😂\": \"face with tears of joy \",\n",
    "    \"😄\": \"grinning face with smiling eyes \",\n",
    "    \"😅\": \"grinning face with sweat \",\n",
    "    \"😆\": \"grinning squinting face \",\n",
    "    \"😇\": \"smiling face with halo \",\n",
    "    \"😞\": \"disappointed face \",\n",
    "    \"😔\": \"pensive face \",\n",
    "    \"😑\": \"expressionless face \",\n",
    "    \"😒\": \"unamused face \",\n",
    "    \"😓\": \"downcast face with sweat \",\n",
    "    \"😕\": \"confused face \",\n",
    "    \"😖\": \"confounded face \",\n",
    "    \"💰\": \"Money Bag \",\n",
    "    \"📈\": \"Up Trend \",\n",
    "    \"🤣\": \"Rolling on the Floor Laughing \",\n",
    "    \"🎊\": \"Confetti Ball \",\n",
    "    \"😭\": \"Loudly Crying \",\n",
    "    \"🙁\": \"Slightly frowning face \",\n",
    "    \"💔\": \"Broken Heart \",\n",
    "    \"😢\": \"Crying Face \",\n",
    "    \"😮\": \"Face with Open Mouth \",\n",
    "    \"😵\": \"Dizzy Face \",\n",
    "    \"🙀\": \"Weary Cat \",\n",
    "    \"😱\": \"Face Screaming in Fear \",\n",
    "    \"❗\": \"Exclamation Mark \",\n",
    "    \"😠\": \"Angry Face \",\n",
    "    \"😡\": \"Pouting Face \",\n",
    "    \"😤\": \"Face with Steam from Nose \",\n",
    "    \"👎\": \"Thumbs Down \",\n",
    "    \"🔪\": \"Hocho \",\n",
    "    \"🌕\": \"Moon \",\n",
    "    \"🚀\": \"Rocket \",\n",
    "    \"💎\": \"Diamond \",\n",
    "    \"👀\": \"Eyes \",\n",
    "    \"💭\": \"Thought Balloon \",\n",
    "    \"📉\": \"Down Trend \",\n",
    "    \"😨\": \"Fearful Face \",\n",
    "    \"😩\": \"Weary Face \",\n",
    "    \"😰\": \"Anxious Face with Fear \",\n",
    "    \"💸\": \"Money with Wings \"\n",
    "}\n",
    "\n",
    "# Emoticon to word conversion function\n",
    "def convert_emoticons_to_words(text):\n",
    "    changed_emoticons = 0  # Variable to count the number of changed emoticons\n",
    "    for emoticon, word in emoticon_dict.items():\n",
    "        while emoticon in text:\n",
    "            text = text.replace(emoticon, word + \" \", 1)\n",
    "            changed_emoticons += 1\n",
    "    return text, changed_emoticons\n",
    "\n",
    "# Apply the function and count emoticons for each row\n",
    "def apply_conversion(text):\n",
    "    converted_text, count = convert_emoticons_to_words(text)\n",
    "    return pd.Series([converted_text, count], index=['converted_text', 'emoticons_count'])\n",
    "\n",
    "conversion_results = dataset['text'].apply(apply_conversion)\n",
    "dataset['converted_text'] = conversion_results['converted_text']\n",
    "dataset['emoticons_count'] = conversion_results['emoticons_count']\n",
    "print(\"Emoticons converted to words in 'converted_text' column.\")\n",
    "print(dataset[['converted_text', 'emoticons_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c647fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ff9d183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal applied separately after the option has been chosen and processed\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# Apply the stopwords cleaning after the loop, once the 'text' column has been updated accordingly\n",
    "dataset['text'] = dataset['converted_text'].apply(cleaning_stopwords)\n",
    "print(\"Stopwords removed from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a9069e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeating words cleaned from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to clean repeating words\n",
    "def cleaning_repeating_words(text):\n",
    "    # This regex pattern targets whole words that are repeated\n",
    "    return re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "\n",
    "# Assuming 'dataset' is a pandas DataFrame and 'text' is a column in it\n",
    "# Apply the cleaning function for repeating words to each row in the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(cleaning_repeating_words)\n",
    "print(\"Repeating words cleaned from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7e1f0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    btc on glp resistance for now play safe if u r...\n",
       "1                 hay bullfrog breakout lets fill wick\n",
       "2    did guys see pitch deck reaching community tha...\n",
       "3    gn fam going early bed since am morning nonsto...\n",
       "4    you think week fun?!? face tears joy face tear...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "92c75347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The pattern matches word characters (\\w) and punctuation marks ([^\\w\\s])\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "\n",
    "# Applying the modified tokenizer to the dataset\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f63a0715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d0802f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ffd9e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 2s 43ms/step - loss: 1.0962 - accuracy: 0.4017\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 1.0624 - accuracy: 0.7009\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 1.0157 - accuracy: 0.6752\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.9133 - accuracy: 0.6752\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 40ms/step - loss: 0.8016 - accuracy: 0.6752\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 2s 24ms/step - loss: 1.9412 - accuracy: 0.1966\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 1.8993 - accuracy: 0.4615\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 1.8140 - accuracy: 0.4701\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 1.6543 - accuracy: 0.3504\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 1.5442 - accuracy: 0.3419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x157068250>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All necessary imports should be placed at the top of the script\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the tokenizer with your dataset\n",
    "# NOTE: Ensure `dataset` is correctly loaded with 'text' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "\n",
    "# Function to create LSTM model\n",
    "def create_lstm_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to extract features from LSTM model\n",
    "def extract_features(model, data):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    return intermediate_layer_model.predict(data)\n",
    "\n",
    "# Assuming `dataset` is a pandas DataFrame with 'text', 'polarity', and 'emotion' columns\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "# Convert polarity and emotion labels to one-hot encoding\n",
    "encoder_polarity = LabelEncoder()\n",
    "y_polarity = to_categorical(encoder_polarity.fit_transform(dataset['polarity']))\n",
    "\n",
    "encoder_emotion = LabelEncoder()\n",
    "y_emotion = to_categorical(encoder_emotion.fit_transform(dataset['emotion']))\n",
    "\n",
    "# Split the data into training and testing sets for polarity and emotion\n",
    "X_train_polarity, X_test_polarity, y_train_polarity, y_test_polarity = train_test_split(X, y_polarity, test_size=0.2, random_state=42)\n",
    "X_train_emotion, X_test_emotion, y_train_emotion, y_test_emotion = train_test_split(X, y_emotion, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train LSTM models for polarity and emotion\n",
    "lstm_model_polarity = create_lstm_model(50, y_polarity.shape[1])\n",
    "lstm_model_emotion = create_lstm_model(50, y_emotion.shape[1])\n",
    "\n",
    "lstm_model_polarity.fit(X_train_polarity, y_train_polarity, epochs=5, batch_size=32)\n",
    "lstm_model_emotion.fit(X_train_emotion, y_train_emotion, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dfcb89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features with LSTM for SVM training\n",
    "X_train_features_polarity = extract_features(lstm_model_polarity, X_train_polarity)\n",
    "X_train_features_emotion = extract_features(lstm_model_emotion, X_train_emotion)\n",
    "\n",
    "# Train SVM for polarity and emotion\n",
    "svm_classifier_polarity = SVC(kernel='linear', probability=True)\n",
    "svm_classifier_emotion = SVC(kernel='linear', probability=True)\n",
    "\n",
    "svm_classifier_polarity.fit(X_train_features_polarity, np.argmax(y_train_polarity, axis=1))\n",
    "svm_classifier_emotion.fit(X_train_features_emotion, np.argmax(y_train_emotion, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "45cf3e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 393ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "Tweet: I'm angry!!!\n",
      "Polarity Label: negative\n",
      "Emotion Label: anticipation\n",
      "Intensity Level: High\n"
     ]
    }
   ],
   "source": [
    "# Assuming `polarity_labels` is your list of original labels for the training data\n",
    "# Example list of polarity labels used in your training dataset\n",
    "polarity_labels = ['positive', 'negative', 'neutral']  # This should be replaced by the actual labels you have\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder_polarity = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder with your actual labels\n",
    "encoder_polarity.fit(polarity_labels)\n",
    "\n",
    "# Assuming encoder is a pre-defined LabelEncoder object for decoding the emotion labels\n",
    "\n",
    "# This function should extract LSTM features\n",
    "def extract_features(model, sequence):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    return intermediate_layer_model.predict(sequence)\n",
    "\n",
    "# Function to classify the intensity\n",
    "def classify_intensity(emoticons_count, text):\n",
    "    question_marks = text.count('?')\n",
    "    periods = text.count('.')\n",
    "    exclamation_marks = text.count('!')\n",
    "\n",
    "    if exclamation_marks > 1 or question_marks > 1 or emoticons_count > 1:\n",
    "        return 'High'\n",
    "    elif periods == 1 or question_marks == 1 or emoticons_count == 1 or exclamation_marks ==1 :\n",
    "        return 'Medium'\n",
    "    elif question_marks == 0 and emoticons_count == 0:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Undetermined'\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "    return ' '.join(tokenizer.tokenize(text))\n",
    "\n",
    "#Function to perform real-time prediction and intensity classification\n",
    "def real_time_prediction(text, tokenizer, lstm_model_emotion, lstm_model_polarity, svm_classifier_emotion, svm_classifier_polarity, encoder_emotion, encoder_polarity):\n",
    "    # Preprocessing steps (assuming these functions are defined elsewhere in your code)\n",
    "    \n",
    "    # Assuming we have a single text input and all necessary models, tokenizers, and encoders\n",
    "    tweet = \"I'm happy!!!\"\n",
    "\n",
    "    cleaned_text = cleaning_numbers(text)\n",
    "    cleaned_tweet = clean_tweet(cleaned_text)\n",
    "    corrected_text = spell_correction(cleaned_tweet)\n",
    "    emoticon_converted_text, emoticons_count = convert_emoticons_to_words(corrected_text)  # Ensure this function returns emoticons_count\n",
    "    cleaned_stopwords = cleaning_stopwords(emoticon_converted_text)\n",
    "    cleaned_repeating_words = cleaning_repeating_words(cleaned_stopwords)\n",
    "\n",
    "    # Now tokenize the text after cleaning repeating words\n",
    "    tokenized_text = tokenize_text(cleaned_repeating_words)\n",
    "\n",
    "    # Continue with any additional preprocessing steps that work on the tokenized text\n",
    "    stemmed_text = stemming_on_text(tokenized_text)\n",
    "    lemmatized_text = lemmatizer_on_text(stemmed_text)\n",
    "\n",
    "    assert isinstance(lemmatized_text, str), \"Processed text must be a string\"\n",
    "\n",
    "    # Convert the processed text to a sequence\n",
    "    sequence = tokenizer.texts_to_sequences([lemmatized_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=50)\n",
    "\n",
    "\n",
    "    # Predict emotion and polarity using the LSTM model\n",
    "    lstm_prediction_emotion = lstm_model_emotion.predict(padded_sequence)\n",
    "    lstm_features_emotion = extract_features(lstm_model_emotion, padded_sequence)\n",
    "    svm_prediction_emotion = svm_classifier_emotion.predict_proba(lstm_features_emotion)\n",
    "\n",
    "    lstm_prediction_polarity = lstm_model_polarity.predict(padded_sequence)\n",
    "    lstm_features_polarity = extract_features(lstm_model_polarity, padded_sequence)\n",
    "    svm_prediction_polarity = svm_classifier_polarity.predict_proba(lstm_features_polarity)\n",
    "\n",
    "    # Decode the predicted labels\n",
    "    emotion_label = encoder_emotion.inverse_transform(np.argmax(lstm_prediction_emotion, axis=1))\n",
    "    polarity_label = encoder_polarity.inverse_transform(np.argmax(svm_prediction_polarity, axis=1))\n",
    "\n",
    "    # Get probabilities for the predicted labels\n",
    "    emotion_probability = np.max(lstm_prediction_emotion, axis=1)\n",
    "    polarity_probability = np.max(svm_prediction_polarity, axis=1)\n",
    "\n",
    "    # Classify the intensity\n",
    "    intensity = classify_intensity(emoticons_count, text)  # Ensure `emoticons_count` is defined\n",
    "\n",
    "    return polarity_label, emotion_label, polarity_probability, emotion_probability, intensity\n",
    "\n",
    "\n",
    "# Call the real-time prediction function\n",
    "polarity_label, emotion_label, polarity_probability, emotion_probability, intensity = real_time_prediction(\n",
    "    tweet, tokenizer, lstm_model_emotion, lstm_model_polarity, svm_classifier_emotion, svm_classifier_polarity, encoder_emotion, encoder_polarity)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Tweet: {tweet}\")\n",
    "print(f\"Polarity Label: {polarity_label[0]}\")\n",
    "print(f\"Emotion Label: {emotion_label[0]}\")\n",
    "print(f\"Intensity Level: {intensity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cc28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

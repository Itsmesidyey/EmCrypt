{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2a48e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#SpellCorrection\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import string\n",
    "import emoji\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7ec4dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: UTF-8-SIG\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4:46 PM ¬∑ Oct 23, 2023</td>\n",
       "      <td>@96Ssandro</td>\n",
       "      <td>The first bullish signal is to close above the...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>10/30/23</td>\n",
       "      <td>@yourcryptodj</td>\n",
       "      <td>Shill me your moon bag üí∞#Crypto</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1:34 AM ¬∑ Oct 25, 2023</td>\n",
       "      <td>@Mrsamiullah55</td>\n",
       "      <td>Bad News for the #crypto market.\\n\\nüìåThe ticke...</td>\n",
       "      <td>1</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4:34 AM ¬∑ Oct 25, 2023</td>\n",
       "      <td>@MtunezOfficial</td>\n",
       "      <td>Amazing times for \"Dogestronauts\"üê∂ üë®‚ÄçüöÄ üöÄ\\n\\nIt...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>10/27/23</td>\n",
       "      <td>@Netcoins</td>\n",
       "      <td>Curious about crypto #staking and earning rewa...</td>\n",
       "      <td>1</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date         username  \\\n",
       "142  4:46 PM ¬∑ Oct 23, 2023       @96Ssandro   \n",
       "556                10/30/23    @yourcryptodj   \n",
       "287  1:34 AM ¬∑ Oct 25, 2023   @Mrsamiullah55   \n",
       "60   4:34 AM ¬∑ Oct 25, 2023  @MtunezOfficial   \n",
       "547                10/27/23        @Netcoins   \n",
       "\n",
       "                                                  text  polarity  emotion  \n",
       "142  The first bullish signal is to close above the...         2    happy  \n",
       "556                    Shill me your moon bag üí∞#Crypto         1  Neutral  \n",
       "287  Bad News for the #crypto market.\\n\\nüìåThe ticke...         1      sad  \n",
       "60   Amazing times for \"Dogestronauts\"üê∂ üë®‚ÄçüöÄ üöÄ\\n\\nIt...         2    happy  \n",
       "547  Curious about crypto #staking and earning rewa...         1  Neutral  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['date', 'username', 'text', 'polarity', 'emotion']\n",
    "\n",
    "#Detect file encoding using chardet\n",
    "with open('data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Print the detected encoding\n",
    "print(\"Detected encoding:\", result['encoding'])\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "df = pd.read_csv('data.csv', encoding=result['encoding'], names=DATASET_COLUMNS)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "10f5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "data=df[['text','polarity', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ed21aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2ba51c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['polarity'] == 2]\n",
    "data_neu = data[data['polarity'] == 1]\n",
    "data_neg = data[data['polarity'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b6d97b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(100)]\n",
    "data_neu = data_neu.iloc[:int(100)]\n",
    "data_neg = data_neg.iloc[:int(100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0888a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([data_pos, data_neu, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "b8bd68dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450    Hey Websec fam, We're collaborating with @jobs...\n",
       "452    Shill me your next Big #Crypto ready to Explod...\n",
       "457    The eyes of the Geishas...watching you. Why ar...\n",
       "461    What is the concept that a blockchain cannot a...\n",
       "462    Web innovation: endless opportunities. #crypto...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3469fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1            HAY bullflag breakoutüëÄ Lets fill that wicküöÄ\n",
      "2      Did you guys see how is doing a pitch with a d...\n",
      "3      GN Fam going early to bed been up since or AM ...\n",
      "4      You think this week has been fun?!? üòÇüòÇüòÇüòÇ Wait ...\n",
      "                             ...                        \n",
      "450    Hey Websec fam Were collaborating with Get top...\n",
      "452            Shill me your next Big ready to ExplodeüöÄüìà\n",
      "457    The eyes of the Geishas...watching you. Why ar...\n",
      "461    What is the concept that a blockchain cannot a...\n",
      "462                Web innovation endless opportunities.\n",
      "Name: text, Length: 200, dtype: object\n"
     ]
    }
   ],
   "source": [
    "emoticons_to_keep = [\n",
    "    'üí∞', 'üìà', 'ü§£', 'üéä', 'üòÇ', 'üò≠', 'üôÅ', 'üòû', 'üíî', 'üò¢', 'üòÆ', 'üòµ', 'üôÄ',\n",
    "    'üò±', '‚ùó', 'üò†', 'üò°', 'üò§', 'üëé', 'üî™', 'üåï', 'üöÄ', 'üíé', 'üëÄ', 'üí≠', 'üìâ',\n",
    "    'üò®', 'üò©', 'üò∞', 'üí∏'\n",
    "]\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove special characters except for emoticons\n",
    "    text = re.sub(r'[^\\w\\s.!?{}]+'.format(''.join(emoticons_to_keep)), '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the modified cleaning function to the 'text' column in your dataset\n",
    "dataset['text'] = dataset['text'].apply(clean_tweet)\n",
    "\n",
    "# Display the 'text' column in the entire dataset\n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9db4387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  polarity       emotion\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...         2         happy\n",
      "1            HAY bullfrog breakout Lets fill that wick         2  anticipation\n",
      "2    Did you guys see how is doing a pitch with a d...         2         happy\n",
      "3    GN Fam going early to bed been up since or AM ...         2         happy\n",
      "4    You think this week has been fun?!? üòÇüòÇüòÇüòÇ Wait ...         2  anticipation\n",
      "..                                                 ...       ...           ...\n",
      "450  Hey Websec fam Were collaborating with Get top...         1       Neutral\n",
      "452          Shill me your next Big ready to ExplodeüöÄüìà         1       Neutral\n",
      "457  The eyes of the Geishas...watching you Why are...         1       Neutral\n",
      "461  What is the concept that a blockchain cannot a...         1       Neutral\n",
      "462               Web innovation endless opportunities         1       Neutral\n",
      "\n",
      "[200 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SpellChecker only once to avoid re-creation for each call\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function for spell correction\n",
    "def spell_correction(text):\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Check if the correction is not None, otherwise use the original word\n",
    "            corrected_words.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply spell correction to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(spell_correction)\n",
    "\n",
    "# Display the entire dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "52520eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons converted to words in 'converted_text' column.\n",
      "                                      converted_text  emoticons_count\n",
      "0  BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...                0\n",
      "1          HAY bullfrog breakout Lets fill that wick                0\n",
      "2  Did you guys see how is doing a pitch with a d...                0\n",
      "3  GN Fam going early to bed been up since or AM ...                0\n",
      "4  You think this week has been fun?!? face with ...               13\n"
     ]
    }
   ],
   "source": [
    "#Define the emoticon dictionary outside the function for a wider scope\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smile \",\n",
    "    \":(\": \"sad \",\n",
    "    \":D\": \"laugh \",\n",
    "    \"üòä\": \"smiling face with smiling eyes \",\n",
    "    \"üòÉ\": \"grinning face with big eyes \",\n",
    "    \"üòâ\": \"winking face \",\n",
    "    \"üëå\": \"OK hand \",\n",
    "    \"üëç\": \"Thumbs up \",\n",
    "    \"üòÅ\": \"beaming face with smiling eyes \",\n",
    "    \"üòÇ\": \"face with tears of joy \",\n",
    "    \"üòÑ\": \"grinning face with smiling eyes \",\n",
    "    \"üòÖ\": \"grinning face with sweat \",\n",
    "    \"üòÜ\": \"grinning squinting face \",\n",
    "    \"üòá\": \"smiling face with halo \",\n",
    "    \"üòû\": \"disappointed face \",\n",
    "    \"üòî\": \"pensive face \",\n",
    "    \"üòë\": \"expressionless face \",\n",
    "    \"üòí\": \"unamused face \",\n",
    "    \"üòì\": \"downcast face with sweat \",\n",
    "    \"üòï\": \"confused face \",\n",
    "    \"üòñ\": \"confounded face \",\n",
    "    \"üí∞\": \"Money Bag \",\n",
    "    \"üìà\": \"Up Trend \",\n",
    "    \"ü§£\": \"Rolling on the Floor Laughing \",\n",
    "    \"üéä\": \"Confetti Ball \",\n",
    "    \"üò≠\": \"Loudly Crying \",\n",
    "    \"üôÅ\": \"Slightly frowning face \",\n",
    "    \"üíî\": \"Broken Heart \",\n",
    "    \"üò¢\": \"Crying Face \",\n",
    "    \"üòÆ\": \"Face with Open Mouth \",\n",
    "    \"üòµ\": \"Dizzy Face \",\n",
    "    \"üôÄ\": \"Weary Cat \",\n",
    "    \"üò±\": \"Face Screaming in Fear \",\n",
    "    \"‚ùó\": \"Exclamation Mark \",\n",
    "    \"üò†\": \"Angry Face \",\n",
    "    \"üò°\": \"Pouting Face \",\n",
    "    \"üò§\": \"Face with Steam from Nose \",\n",
    "    \"üëé\": \"Thumbs Down \",\n",
    "    \"üî™\": \"Hocho \",\n",
    "    \"üåï\": \"Moon \",\n",
    "    \"üöÄ\": \"Rocket \",\n",
    "    \"üíé\": \"Diamond \",\n",
    "    \"üëÄ\": \"Eyes \",\n",
    "    \"üí≠\": \"Thought Balloon \",\n",
    "    \"üìâ\": \"Down Trend \",\n",
    "    \"üò®\": \"Fearful Face \",\n",
    "    \"üò©\": \"Weary Face \",\n",
    "    \"üò∞\": \"Anxious Face with Fear \",\n",
    "    \"üí∏\": \"Money with Wings \"\n",
    "}\n",
    "\n",
    "# Emoticon to word conversion function\n",
    "def convert_emoticons_to_words(text):\n",
    "    changed_emoticons = 0  # Variable to count the number of changed emoticons\n",
    "    for emoticon, word in emoticon_dict.items():\n",
    "        while emoticon in text:\n",
    "            text = text.replace(emoticon, word + \" \", 1)\n",
    "            changed_emoticons += 1\n",
    "    return text, changed_emoticons\n",
    "\n",
    "# Apply the function and count emoticons for each row\n",
    "def apply_conversion(text):\n",
    "    converted_text, count = convert_emoticons_to_words(text)\n",
    "    return pd.Series([converted_text, count], index=['converted_text', 'emoticons_count'])\n",
    "\n",
    "conversion_results = dataset['text'].apply(apply_conversion)\n",
    "dataset['converted_text'] = conversion_results['converted_text']\n",
    "dataset['emoticons_count'] = conversion_results['emoticons_count']\n",
    "print(\"Emoticons converted to words in 'converted_text' column.\")\n",
    "print(dataset[['converted_text', 'emoticons_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c647fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ff9d183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal applied separately after the option has been chosen and processed\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# Apply the stopwords cleaning after the loop, once the 'text' column has been updated accordingly\n",
    "dataset['text'] = dataset['converted_text'].apply(cleaning_stopwords)\n",
    "print(\"Stopwords removed from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a9069e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeating words cleaned from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to clean repeating words\n",
    "def cleaning_repeating_words(text):\n",
    "    # This regex pattern targets whole words that are repeated\n",
    "    return re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "\n",
    "# Assuming 'dataset' is a pandas DataFrame and 'text' is a column in it\n",
    "# Apply the cleaning function for repeating words to each row in the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(cleaning_repeating_words)\n",
    "print(\"Repeating words cleaned from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "7e1f0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    btc on glp resistance for now play safe if u r...\n",
       "1                 hay bullfrog breakout lets fill wick\n",
       "2    did guys see pitch deck reaching community tha...\n",
       "3    gn fam going early bed since am morning nonsto...\n",
       "4    you think week fun?!? face tears joy face tear...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "92c75347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The pattern matches word characters (\\w) and punctuation marks ([^\\w\\s])\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "\n",
    "# Applying the modified tokenizer to the dataset\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f63a0715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "d0802f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ffd9e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 3s 309ms/step - loss: 0.6937 - accuracy: 0.4750 - val_loss: 0.6966 - val_accuracy: 0.4250\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 70ms/step - loss: 0.6924 - accuracy: 0.5437 - val_loss: 0.6962 - val_accuracy: 0.4250\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 68ms/step - loss: 0.6925 - accuracy: 0.5562 - val_loss: 0.6958 - val_accuracy: 0.4000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 0.6918 - accuracy: 0.5125 - val_loss: 0.6954 - val_accuracy: 0.4250\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 0.6905 - accuracy: 0.5625 - val_loss: 0.6950 - val_accuracy: 0.4250\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 72ms/step - loss: 0.6911 - accuracy: 0.5437 - val_loss: 0.6946 - val_accuracy: 0.4250\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 0.6895 - accuracy: 0.6000 - val_loss: 0.6942 - val_accuracy: 0.3750\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 88ms/step - loss: 0.6892 - accuracy: 0.6500 - val_loss: 0.6937 - val_accuracy: 0.4250\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.6884 - accuracy: 0.6125 - val_loss: 0.6934 - val_accuracy: 0.4500\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.6845 - accuracy: 0.7375 - val_loss: 0.6929 - val_accuracy: 0.4500\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 5s 343ms/step - loss: 2.0783 - accuracy: 0.0938 - val_loss: 2.0792 - val_accuracy: 0.1250\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 82ms/step - loss: 2.0774 - accuracy: 0.1312 - val_loss: 2.0776 - val_accuracy: 0.2000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 2.0735 - accuracy: 0.2062 - val_loss: 2.0760 - val_accuracy: 0.2000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 76ms/step - loss: 2.0719 - accuracy: 0.1875 - val_loss: 2.0745 - val_accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 2.0709 - accuracy: 0.2562 - val_loss: 2.0731 - val_accuracy: 0.3000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 80ms/step - loss: 2.0661 - accuracy: 0.2500 - val_loss: 2.0716 - val_accuracy: 0.3250\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 77ms/step - loss: 2.0652 - accuracy: 0.3000 - val_loss: 2.0701 - val_accuracy: 0.3000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 78ms/step - loss: 2.0631 - accuracy: 0.3063 - val_loss: 2.0687 - val_accuracy: 0.3250\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 83ms/step - loss: 2.0576 - accuracy: 0.4250 - val_loss: 2.0672 - val_accuracy: 0.3250\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 73ms/step - loss: 2.0576 - accuracy: 0.3500 - val_loss: 2.0657 - val_accuracy: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Note that we're now using `tensorflow.keras` instead of `keras` directly\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure `dataset` is correctly loaded with 'text' column\n",
    "# dataset = pd.read_csv('your_dataset.csv') # Uncomment and set your dataset path\n",
    "# Make sure 'text', 'polarity', and 'emotion' columns exist\n",
    "# print(dataset.columns) # Uncomment to check columns\n",
    "\n",
    "# Initialize the tokenizer with your dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "\n",
    "# Function to create LSTM model, now with optimizer initialization inside\n",
    "def create_lstm_model(input_length, num_classes):\n",
    "    # Initialize the optimizer with the desired settings inside the function\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # The optimizer is used here directly after its initialization\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Adjust the learning rate\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Prepare the dataset for training\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder_polarity = LabelEncoder()\n",
    "y_polarity = to_categorical(encoder_polarity.fit_transform(dataset['polarity']))\n",
    "\n",
    "encoder_emotion = LabelEncoder()\n",
    "y_emotion = to_categorical(encoder_emotion.fit_transform(dataset['emotion']))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_polarity, X_test_polarity, y_train_polarity, y_test_polarity = train_test_split(X, y_polarity, test_size=0.2, random_state=42)\n",
    "X_train_emotion, X_test_emotion, y_train_emotion, y_test_emotion = train_test_split(X, y_emotion, test_size=0.2, random_state=42)\n",
    "\n",
    "# When creating the models, we do not pass the optimizer as a parameter anymore\n",
    "lstm_model_polarity = create_lstm_model(50, y_polarity.shape[1])\n",
    "lstm_model_emotion = create_lstm_model(50, y_emotion.shape[1])\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the LSTM models with EarlyStopping and validation data\n",
    "lstm_model_polarity.fit(\n",
    "    X_train_polarity, y_train_polarity, \n",
    "    epochs=10, \n",
    "    batch_size=64,  # Ensure batch size divides the number of samples evenly or set shuffle=True\n",
    "    validation_data=(X_test_polarity, y_test_polarity),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "lstm_model_emotion.fit(\n",
    "    X_train_emotion, y_train_emotion, \n",
    "    epochs=10, \n",
    "    batch_size=64,  # Same batch size for consistency and ease of computation\n",
    "    validation_data=(X_test_emotion, y_test_emotion),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save the trained LSTM models\n",
    "lstm_model_polarity.save('lstm_polarity_model.h5')\n",
    "lstm_model_emotion.save('lstm_emotion_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "dfcb89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 8ms/step\n",
      "5/5 [==============================] - 0s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features with LSTM for SVM training\n",
    "X_train_features_polarity = extract_features(lstm_model_polarity, X_train_polarity)\n",
    "X_train_features_emotion = extract_features(lstm_model_emotion, X_train_emotion)\n",
    "\n",
    "# Train SVM for polarity and emotion\n",
    "svm_classifier_polarity = SVC(kernel='linear', probability=True)\n",
    "svm_classifier_emotion = SVC(kernel='linear', probability=True)\n",
    "\n",
    "svm_classifier_polarity.fit(X_train_features_polarity, np.argmax(y_train_polarity, axis=1))\n",
    "svm_classifier_emotion.fit(X_train_features_emotion, np.argmax(y_train_emotion, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "45cf3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_emotion_model.joblib']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained SVM models using joblib or pickle\n",
    "import joblib\n",
    "joblib.dump(svm_classifier_polarity, 'svm_polarity_model.joblib')\n",
    "joblib.dump(svm_classifier_emotion, 'svm_emotion_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "843cc28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 20ms/step\n",
      "Polarity Precision: 0.4540\n",
      "Polarity Recall: 0.4500\n",
      "Polarity F1 Score: 0.4417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set for polarity\n",
    "y_pred_polarity = lstm_model_polarity.predict(X_test_polarity)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_polarity = np.argmax(y_pred_polarity, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_polarity = np.argmax(y_test_polarity, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for polarity\n",
    "precision_polarity = precision_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "recall_polarity = recall_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "f1_score_polarity = f1_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "\n",
    "print(f'Polarity Precision: {precision_polarity:.4f}')\n",
    "print(f'Polarity Recall: {recall_polarity:.4f}')\n",
    "print(f'Polarity F1 Score: {f1_score_polarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b4e3a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n",
      "Emotion Precision: 0.2348\n",
      "Emotion Recall: 0.3250\n",
      "Emotion F1 Score: 0.2725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set for emotion\n",
    "y_pred_emotion = lstm_model_emotion.predict(X_test_emotion)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_emotion = np.argmax(y_pred_emotion, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_emotion = np.argmax(y_test_emotion, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for emotion\n",
    "precision_emotion = precision_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "recall_emotion = recall_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "f1_score_emotion = f1_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "\n",
    "print(f'Emotion Precision: {precision_emotion:.4f}')\n",
    "print(f'Emotion Recall: {recall_emotion:.4f}')\n",
    "print(f'Emotion F1 Score: {f1_score_emotion:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

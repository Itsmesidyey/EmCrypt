{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a48e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#SpellCorrection\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import string\n",
    "import emoji\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ec4dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: UTF-8-SIG\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5:39 PM Â· Oct 23, 2023</td>\n",
       "      <td>@realChokiie</td>\n",
       "      <td>2/2\\n\\n4) when ETF SPOT will be approved then ...</td>\n",
       "      <td>2</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1:10 PM Â· Oct 24, 2023</td>\n",
       "      <td>@NaijaExcellence</td>\n",
       "      <td>ğŸ‘‡ğŸ½ğŸ‘‡ğŸ½It's only in crypto that you can be fearfu...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7:54 AM Â· Oct 26, 2023</td>\n",
       "      <td>@crypto_chin</td>\n",
       "      <td>$HAY bullflag breakoutğŸ‘€\\n\\nLets fill that wickğŸš€â³</td>\n",
       "      <td>2</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>7:28 PM Â· Oct 25, 2023</td>\n",
       "      <td>@amonbuy</td>\n",
       "      <td>At some point nobody will be able to ignore th...</td>\n",
       "      <td>2</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>9:43 PM Â· Oct 23, 2023</td>\n",
       "      <td>@trendguards</td>\n",
       "      <td>ğŸŸ¢ğŸ“ˆ Green vibes in the market today! ğŸš€ğŸ’š\\n\\nIt's...</td>\n",
       "      <td>2</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date          username  \\\n",
       "44   5:39 PM Â· Oct 23, 2023      @realChokiie   \n",
       "276  1:10 PM Â· Oct 24, 2023  @NaijaExcellence   \n",
       "1    7:54 AM Â· Oct 26, 2023      @crypto_chin   \n",
       "118  7:28 PM Â· Oct 25, 2023          @amonbuy   \n",
       "53   9:43 PM Â· Oct 23, 2023      @trendguards   \n",
       "\n",
       "                                                  text  polarity       emotion  \n",
       "44   2/2\\n\\n4) when ETF SPOT will be approved then ...         2  anticipation  \n",
       "276  ğŸ‘‡ğŸ½ğŸ‘‡ğŸ½It's only in crypto that you can be fearfu...         2         happy  \n",
       "1     $HAY bullflag breakoutğŸ‘€\\n\\nLets fill that wickğŸš€â³         2  anticipation  \n",
       "118  At some point nobody will be able to ignore th...         2  anticipation  \n",
       "53   ğŸŸ¢ğŸ“ˆ Green vibes in the market today! ğŸš€ğŸ’š\\n\\nIt's...         2         happy  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['date', 'username', 'text', 'polarity', 'emotion']\n",
    "\n",
    "#Detect file encoding using chardet\n",
    "with open('data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Print the detected encoding\n",
    "print(\"Detected encoding:\", result['encoding'])\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "df = pd.read_csv('data.csv', encoding=result['encoding'], names=DATASET_COLUMNS)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10f5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "data=df[['text','polarity', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8bd68dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582                Which project has strong community? ğŸš€\n",
       "584    New Zealand Rapper Sesh and DogeCoin Millionai...\n",
       "585    The founder of the bankrupt cryptocurrency exc...\n",
       "586    Unlock the Future with .mmit Domains! Join ove...\n",
       "595    If you sleep now you will have a dream but if ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3469fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1            HAY bullflag breakoutğŸ‘€ Lets fill that wickğŸš€\n",
      "2      Did you guys see how is doing a pitch with a d...\n",
      "3      GN Fam going early to bed been up since or AM ...\n",
      "4      You think this week has been fun?!? ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ Wait ...\n",
      "                             ...                        \n",
      "582                Which project has strong community? ğŸš€\n",
      "584    New Zealand Rapper Sesh and DogeCoin Millionai...\n",
      "585    The founder of the bankrupt cryptocurrency exc...\n",
      "586    Unlock the Future with .mmit Domains! Join ove...\n",
      "595    If you sleep now you will have a dream but if ...\n",
      "Name: text, Length: 347, dtype: object\n"
     ]
    }
   ],
   "source": [
    "emoticons_to_keep = [\n",
    "    'ğŸ’°', 'ğŸ“ˆ', 'ğŸ¤£', 'ğŸŠ', 'ğŸ˜‚', 'ğŸ˜­', 'ğŸ™', 'ğŸ˜', 'ğŸ’”', 'ğŸ˜¢', 'ğŸ˜®', 'ğŸ˜µ', 'ğŸ™€',\n",
    "    'ğŸ˜±', 'â—', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ˜¤', 'ğŸ‘', 'ğŸ”ª', 'ğŸŒ•', 'ğŸš€', 'ğŸ’', 'ğŸ‘€', 'ğŸ’­', 'ğŸ“‰',\n",
    "    'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜°', 'ğŸ’¸'\n",
    "]\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove special characters except for emoticons\n",
    "    text = re.sub(r'[^\\w\\s.!?{}]+'.format(''.join(emoticons_to_keep)), '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the modified cleaning function to the 'text' column in your dataset\n",
    "dataset['text'] = dataset['text'].apply(clean_tweet)\n",
    "\n",
    "# Display the 'text' column in the entire dataset\n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9db4387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  polarity       emotion\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...         2         happy\n",
      "1            HAY bullfrog breakout Lets fill that wick         2  anticipation\n",
      "2    Did you guys see how is doing a pitch with a d...         2         happy\n",
      "3    GN Fam going early to bed been up since or AM ...         2         happy\n",
      "4    You think this week has been fun?!? ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ Wait ...         2  anticipation\n",
      "..                                                 ...       ...           ...\n",
      "582               Which project has strong community i         1         happy\n",
      "584  New Zealand Rapper Sesh and DogeCoin Millionai...         1         happy\n",
      "585  The founder of the bankrupt cryptocurrency exc...         1         happy\n",
      "586  Unlock the Future with emmit Domains! Join ove...         1         happy\n",
      "595  If you sleep now you will have a dream but if ...         1         happy\n",
      "\n",
      "[347 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SpellChecker only once to avoid re-creation for each call\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function for spell correction\n",
    "def spell_correction(text):\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Check if the correction is not None, otherwise use the original word\n",
    "            corrected_words.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply spell correction to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(spell_correction)\n",
    "\n",
    "# Display the entire dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52520eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons converted to words in 'converted_text' column.\n",
      "                                      converted_text  emoticons_count\n",
      "0  BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...                0\n",
      "1          HAY bullfrog breakout Lets fill that wick                0\n",
      "2  Did you guys see how is doing a pitch with a d...                0\n",
      "3  GN Fam going early to bed been up since or AM ...                0\n",
      "4  You think this week has been fun?!? Face with ...               13\n"
     ]
    }
   ],
   "source": [
    "#Define the emoticon dictionary outside the function for a wider scope\n",
    "emoticon_dict = {\n",
    "    \"ğŸŒˆ\": \"Rainbow\",\n",
    "    \"ğŸŒ™\": \"Crescent Moon\",\n",
    "    \"ğŸŒš\": \"New Moon Face\",\n",
    "    \"ğŸŒ\": \"Sun with Face\",\n",
    "    \"ğŸŒŸ\": \"Glowing Star\",\n",
    "    \"ğŸŒ·\": \"Tulip\",\n",
    "    \"ğŸŒ¸\": \"Cherry Blossom\",\n",
    "    \"ğŸŒ¹\": \"Rose\",\n",
    "    \"ğŸŒº\": \"Hibiscus\",\n",
    "    \"ğŸ€\": \"Four Leaf Clover\",\n",
    "    \"ğŸ•\": \"Pizza\",\n",
    "    \"ğŸ»\": \"Clinking Beer Mugs\",\n",
    "    \"ğŸ€\": \"Ribbon\",\n",
    "    \"ğŸˆ\": \"Balloon\",\n",
    "    \"ğŸ‰\": \"Party Popper\",\n",
    "    \"ğŸ¤\": \"Microphone\",\n",
    "    \"ğŸ¥\": \"Movie Camera\",\n",
    "    \"ğŸ§\": \"Headphone\",\n",
    "    \"ğŸµ\": \"Musical Note\",\n",
    "    \"ğŸ¶\": \"Musical Notes\",\n",
    "    \"ğŸ‘€\": \"Eyes\",\n",
    "    \"ğŸ‘…\": \"Tongue\",\n",
    "    \"ğŸ‘‡\": \"Backhand Index Pointing Down\",\n",
    "    \"ğŸ‘ˆ\": \"Backhand Index Pointing Left\",\n",
    "    \"ğŸ‘‰\": \"Backhand Index Pointing Right\",\n",
    "    \"ğŸ‘‹\": \"Waving Hand\",\n",
    "    \"ğŸ‘Œ\": \"OK Hand\",\n",
    "    \"ğŸ‘\": \"Thumbs Up\",\n",
    "    \"ğŸ‘\": \"Clapping Hands\",\n",
    "    \"ğŸ‘‘\": \"Crown\",\n",
    "    \"ğŸ’€\": \"Skull\",\n",
    "    \"ğŸ’\": \"Person Tipping Hand\",\n",
    "    \"ğŸ’ƒ\": \"Woman Dancing\",\n",
    "    \"ğŸ’‹\": \"Kiss Mark\",\n",
    "    \"ğŸ’\": \"Gem Stone\",\n",
    "    \"ğŸ’\": \"Bouquet\",\n",
    "    \"ğŸ’“\": \"Beating Heart\",\n",
    "    \"ğŸ’•\": \"Two Hearts\",\n",
    "    \"ğŸ’–\": \"Sparkling Heart\",\n",
    "    \"ğŸ’—\": \"Growing Heart\",\n",
    "    \"ğŸ’˜\": \"Heart with Arrow\",\n",
    "    \"ğŸ’™\": \"Blue Heart\",\n",
    "    \"ğŸ’š\": \"Green Heart\",\n",
    "    \"ğŸ’›\": \"Yellow Heart\",\n",
    "    \"ğŸ’œ\": \"Purple Heart\",\n",
    "    \"ğŸ’\": \"Revolving Hearts\",\n",
    "    \"ğŸ’¤\": \"Zzz\",\n",
    "    \"ğŸ’¥\": \"Collision\",\n",
    "    \"ğŸ’¦\": \"Sweat Droplets\",\n",
    "    \"ğŸ’ª\": \"Flexed Biceps\",\n",
    "    \"ğŸ’«\": \"Dizzy\",\n",
    "    \"ğŸ’¯\": \"Hundred Points\",\n",
    "    \"ğŸ’°\": \"Money Bag\",\n",
    "    \"ğŸ“·\": \"Camera\",\n",
    "    \"ğŸ”¥\": \"Fire\",\n",
    "    \"ğŸ˜€\": \"Grinning Face\",\n",
    "    \"ğŸ˜\": \"Beaming Face with Smiling Eyes\",\n",
    "    \"ğŸ˜‚\": \"Face with Tears of Joy\",\n",
    "    \"ğŸ˜ƒ\": \"Grinning Face with Big Eyes\",\n",
    "    \"ğŸ˜„\": \"Grinning Face with Smiling Eyes\",\n",
    "    \"ğŸ˜…\": \"Grinning Face with Sweat\",\n",
    "    \"ğŸ˜†\": \"Grinning Squinting Face\",\n",
    "    \"ğŸ˜‡\": \"Smiling Face with Halo\",\n",
    "    \"ğŸ˜ˆ\": \"Smiling Face with Horns\",\n",
    "    \"ğŸ˜‰\": \"Winking Face\",\n",
    "    \"ğŸ˜Š\": \"Smiling Face with Smiling Eyes\",\n",
    "    \"ğŸ˜‹\": \"Face Savoring Food\",\n",
    "    \"ğŸ˜Œ\": \"Relieved Face\",\n",
    "    \"ğŸ˜\": \"Smiling Face with Heart-Eyes\",\n",
    "    \"ğŸ˜\": \"Smiling Face with Sunglasses\",\n",
    "    \"ğŸ˜\": \"Smirking Face\",\n",
    "    \"ğŸ˜º\": \"Smiling Cat with Smiling Eyes\",\n",
    "    \"ğŸ˜»\": \"Smiling Cat with Heart-Eyes\",\n",
    "    \"ğŸ˜½\": \"Kissing Cat with Closed Eyes\",\n",
    "    \"ğŸ™€\": \"Weary Cat\",\n",
    "    \"ğŸ™\": \"Folded Hands\",\n",
    "    \"â˜€\": \"Sun\",\n",
    "    \"â˜º\": \"Smiling Face\",\n",
    "    \"â™¥\": \"Heart Suit\",\n",
    "    \"âœ…\": \"Check Mark Button\",\n",
    "    \"âœˆ\": \"Airplane\",\n",
    "    \"âœŠ\": \"Raised Fist\",\n",
    "    \"âœ‹\": \"Raised Hand\",\n",
    "    \"âœŒ\": \"Victory Hand\",\n",
    "    \"âœ”\": \"Check Mark\",\n",
    "    \"âœ¨\": \"Sparkles\",\n",
    "    \"â„\": \"Snowflake\",\n",
    "    \"â¤\": \"Red Heart\",\n",
    "    \"â­\": \"Star\",\n",
    "    \"ğŸ˜¢\": \"Crying Face\",\n",
    "    \"ğŸ˜­\": \"Loudly Crying Face\",\n",
    "    \"ğŸ˜\": \"Disappointed Face\",\n",
    "    \"ğŸ˜Ÿ\": \"Worried Face\",\n",
    "    \"ğŸ˜ \": \"Angry Face\",\n",
    "    \"ğŸ˜¡\": \"Pouting Face\",\n",
    "    \"ğŸ˜”\": \"Pensive Face\",\n",
    "    \"ğŸ˜•\": \"Confused Face\",\n",
    "    \"ğŸ˜–\": \"Confounded Face\",\n",
    "    \"ğŸ˜¨\": \"Fearful Face\",\n",
    "    \"ğŸ˜©\": \"Weary Face\",\n",
    "    \"ğŸ˜ª\": \"Sleepy Face\",\n",
    "    \"ğŸ˜«\": \"Tired Face\",\n",
    "    \"ğŸ˜°\": \"Anxious Face with Sweat\",\n",
    "    \"ğŸ˜±\": \"Face Screaming in Fear\",\n",
    "    \"ğŸ˜³\": \"Flushed Face\",\n",
    "    \"ğŸ˜¶\": \"Face Without Mouth\",\n",
    "    \"ğŸ˜·\": \"Face with Medical Mask\",\n",
    "    \"ğŸ‘Š\": \"Oncoming Fist\",\n",
    "    \"ğŸ‘\": \"Thumbs Down\",\n",
    "    \"âŒ\": \"Cross Mark\",\n",
    "    \"ğŸ˜²\": \"Astonished Face\",\n",
    "    \"ğŸ˜¯\": \"Hushed Face\",\n",
    "    \"ğŸ˜®\": \"Face with Open Mouth\",\n",
    "    \"ğŸ˜µ\": \"Dizzy Face\",\n",
    "    \"ğŸ™Š\": \"Speak-No-Evil Monkey\",\n",
    "    \"ğŸ™‰\": \"Hear-No-Evil Monkey\",\n",
    "    \"ğŸ™ˆ\": \"See-No-Evil Monkey\",\n",
    "    \"ğŸ’­\": \"Thought Balloon\",\n",
    "    \"â—\": \"Exclamation Mark\",\n",
    "    \"âš¡\": \"High Voltage\",\n",
    "    \"ğŸŠ\": \"Confetti Ball\",\n",
    "    \"ğŸ™\": \"Slightly frowning face\",\n",
    "    \"ğŸ’”\": \"Broken Heart\",\n",
    "    \"ğŸ˜¤\": \"Face with Steam from Nose\",\n",
    "    \"ğŸ”ª\": \"Hocho\",\n",
    "    \"ğŸŒ•\": \"Full Moon\",\n",
    "    \"ğŸš€\": \"Rocket\",\n",
    "    \"ğŸ“‰\": \"Down Trend\",\n",
    "    \"ğŸ¤£\": \"Rolling on the Floor Laughing\",\n",
    "    \"ğŸ’¸\": \"Money with Wings\"\n",
    "}\n",
    "\n",
    "# Emoticon to word conversion function\n",
    "def convert_emoticons_to_words(text):\n",
    "    changed_emoticons = 0  # Variable to count the number of changed emoticons\n",
    "    for emoticon, word in emoticon_dict.items():\n",
    "        while emoticon in text:\n",
    "            text = text.replace(emoticon, word + \" \", 1)\n",
    "            changed_emoticons += 1\n",
    "    return text, changed_emoticons\n",
    "\n",
    "# Apply the function and count emoticons for each row\n",
    "def apply_conversion(text):\n",
    "    converted_text, count = convert_emoticons_to_words(text)\n",
    "    return pd.Series([converted_text, count], index=['converted_text', 'emoticons_count'])\n",
    "\n",
    "conversion_results = dataset['text'].apply(apply_conversion)\n",
    "dataset['converted_text'] = conversion_results['converted_text']\n",
    "dataset['emoticons_count'] = conversion_results['emoticons_count']\n",
    "print(\"Emoticons converted to words in 'converted_text' column.\")\n",
    "print(dataset[['converted_text', 'emoticons_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c647fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff9d183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? Face Tears Joy Face Tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal applied separately after the option has been chosen and processed\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# Apply the stopwords cleaning after the loop, once the 'text' column has been updated accordingly\n",
    "dataset['text'] = dataset['converted_text'].apply(cleaning_stopwords)\n",
    "print(\"Stopwords removed from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9069e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeating words cleaned from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? Face Tears Joy Face Tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to clean repeating words\n",
    "def cleaning_repeating_words(text):\n",
    "    # This regex pattern targets whole words that are repeated\n",
    "    return re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "\n",
    "# Assuming 'dataset' is a pandas DataFrame and 'text' is a column in it\n",
    "# Apply the cleaning function for repeating words to each row in the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(cleaning_repeating_words)\n",
    "print(\"Repeating words cleaned from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7e1f0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    btc on glp resistance for now play safe if u r...\n",
       "1                 hay bullfrog breakout lets fill wick\n",
       "2    did guys see pitch deck reaching community tha...\n",
       "3    gn fam going early bed since am morning nonsto...\n",
       "4    you think week fun?!? face tears joy face tear...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92c75347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The pattern matches word characters (\\w) and punctuation marks ([^\\w\\s])\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "\n",
    "# Applying the modified tokenizer to the dataset\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f63a0715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0802f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffd9e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 8s 663ms/step - loss: 0.7344 - accuracy: 0.4344 - val_loss: 0.7376 - val_accuracy: 0.3929\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 259ms/step - loss: 0.7334 - accuracy: 0.4706 - val_loss: 0.7349 - val_accuracy: 0.3929\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 1s 203ms/step - loss: 0.7313 - accuracy: 0.5249 - val_loss: 0.7325 - val_accuracy: 0.4107\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.7303 - accuracy: 0.5339 - val_loss: 0.7303 - val_accuracy: 0.6250\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 1s 198ms/step - loss: 0.7290 - accuracy: 0.5294 - val_loss: 0.7283 - val_accuracy: 0.6250\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 1s 194ms/step - loss: 0.7246 - accuracy: 0.6380 - val_loss: 0.7262 - val_accuracy: 0.6071\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 1s 202ms/step - loss: 0.7260 - accuracy: 0.5701 - val_loss: 0.7246 - val_accuracy: 0.6071\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 1s 205ms/step - loss: 0.7248 - accuracy: 0.5611 - val_loss: 0.7231 - val_accuracy: 0.6071\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 1s 267ms/step - loss: 0.7215 - accuracy: 0.5837 - val_loss: 0.7216 - val_accuracy: 0.6071\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 1s 243ms/step - loss: 0.7199 - accuracy: 0.5792 - val_loss: 0.7201 - val_accuracy: 0.6071\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 1s 258ms/step - loss: 0.7201 - accuracy: 0.5611 - val_loss: 0.7187 - val_accuracy: 0.6071\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 1s 227ms/step - loss: 0.7192 - accuracy: 0.5566 - val_loss: 0.7174 - val_accuracy: 0.6071\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 1s 266ms/step - loss: 0.7211 - accuracy: 0.5520 - val_loss: 0.7165 - val_accuracy: 0.6071\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 1s 261ms/step - loss: 0.7161 - accuracy: 0.5611 - val_loss: 0.7153 - val_accuracy: 0.6071\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 1s 244ms/step - loss: 0.7153 - accuracy: 0.5566 - val_loss: 0.7144 - val_accuracy: 0.6071\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 1s 229ms/step - loss: 0.7150 - accuracy: 0.5520 - val_loss: 0.7136 - val_accuracy: 0.6071\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 1s 204ms/step - loss: 0.7108 - accuracy: 0.5566 - val_loss: 0.7129 - val_accuracy: 0.6071\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 1s 212ms/step - loss: 0.7133 - accuracy: 0.5520 - val_loss: 0.7123 - val_accuracy: 0.6071\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 1s 206ms/step - loss: 0.7110 - accuracy: 0.5520 - val_loss: 0.7114 - val_accuracy: 0.6071\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 1s 199ms/step - loss: 0.7095 - accuracy: 0.5611 - val_loss: 0.7107 - val_accuracy: 0.6071\n",
      "9/9 [==============================] - 1s 41ms/step\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END ...C=1, gamma=scale, kernel=linear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END ...C=1, gamma=scale, kernel=linear;, score=0.875 total time=   0.0s\n",
      "[CV 3/5] END ...C=1, gamma=scale, kernel=linear;, score=0.909 total time=   0.0s\n",
      "[CV 4/5] END ...C=1, gamma=scale, kernel=linear;, score=0.745 total time=   0.0s\n",
      "[CV 5/5] END ...C=1, gamma=scale, kernel=linear;, score=0.709 total time=   0.0s\n",
      "[CV 1/5] END ..C=10, gamma=scale, kernel=linear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END ..C=10, gamma=scale, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 3/5] END ..C=10, gamma=scale, kernel=linear;, score=0.927 total time=   0.0s\n",
      "[CV 4/5] END ..C=10, gamma=scale, kernel=linear;, score=0.745 total time=   0.0s\n",
      "[CV 5/5] END ..C=10, gamma=scale, kernel=linear;, score=0.709 total time=   0.0s\n",
      "[CV 1/5] END .C=100, gamma=scale, kernel=linear;, score=0.804 total time=   0.0s\n",
      "[CV 2/5] END .C=100, gamma=scale, kernel=linear;, score=0.857 total time=   0.0s\n",
      "[CV 3/5] END .C=100, gamma=scale, kernel=linear;, score=0.927 total time=   0.0s\n",
      "[CV 4/5] END .C=100, gamma=scale, kernel=linear;, score=0.745 total time=   0.0s\n",
      "[CV 5/5] END .C=100, gamma=scale, kernel=linear;, score=0.709 total time=   0.0s\n",
      "3/3 [==============================] - 0s 20ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.50      0.53        26\n",
      "           1       0.72      0.77      0.75        44\n",
      "\n",
      "    accuracy                           0.67        70\n",
      "   macro avg       0.64      0.64      0.64        70\n",
      "weighted avg       0.66      0.67      0.67        70\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_model.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "\n",
    "# Assume dataset is loaded and has columns 'text', 'polarity', and 'emotion'\n",
    "\n",
    "# Initialize the tokenizer with your dataset\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "\n",
    "# Preparing the dataset for training\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "X = pad_sequences(sequences, maxlen=100)  # Increase maxlen if needed\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "encoder_polarity = LabelEncoder()\n",
    "y_polarity = to_categorical(encoder_polarity.fit_transform(dataset['polarity']))\n",
    "\n",
    "encoder_emotion = LabelEncoder()\n",
    "y_emotion = to_categorical(encoder_emotion.fit_transform(dataset['emotion']))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_polarity, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create LSTM model\n",
    "def create_lstm_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model(100, y_train.shape[1])\n",
    "\n",
    "# Early Stopping and Model Checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model_checkpoint = ModelCheckpoint('best_lstm_model.h5', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=20, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "lstm_model.load_weights('best_lstm_model.h5')\n",
    "\n",
    "# Extract features for SVM training\n",
    "intermediate_layer_model = Model(inputs=lstm_model.input, outputs=lstm_model.layers[-3].output)\n",
    "X_train_features = intermediate_layer_model.predict(X_train)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler().fit(X_train_features)\n",
    "X_train_features = scaler.transform(X_train_features)\n",
    "\n",
    "# Grid Search for SVM\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': ['scale'],\n",
    "    'kernel': ['linear']\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(probability=True), param_grid, refit=True, verbose=3)\n",
    "grid_search.fit(X_train_features, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Best SVM estimator\n",
    "svm_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate SVM on test set\n",
    "X_test_features = scaler.transform(intermediate_layer_model.predict(X_test))\n",
    "y_pred = svm_classifier.predict(X_test_features)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))\n",
    "\n",
    "# Save SVM model\n",
    "joblib.dump(svm_classifier, 'svm_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2183a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed classification report:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/cjcasinsinan/Documents/GitHub/EmCrypt/Emcrypt - Clean Version.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cjcasinsinan/Documents/GitHub/EmCrypt/Emcrypt%20-%20Clean%20Version.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# ...\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cjcasinsinan/Documents/GitHub/EmCrypt/Emcrypt%20-%20Clean%20Version.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDetailed classification report:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cjcasinsinan/Documents/GitHub/EmCrypt/Emcrypt%20-%20Clean%20Version.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(np\u001b[39m.\u001b[39;49margmax(y_test, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), y_pred, target_names\u001b[39m=\u001b[39;49mencoder_polarity\u001b[39m.\u001b[39;49mclasses_))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2567\u001b[0m, in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2561\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2562\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlabels size, \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m, does not match size of target_names, \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2563\u001b[0m                 \u001b[39mlen\u001b[39m(labels), \u001b[39mlen\u001b[39m(target_names)\n\u001b[1;32m   2564\u001b[0m             )\n\u001b[1;32m   2565\u001b[0m         )\n\u001b[1;32m   2566\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2567\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2568\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNumber of classes, \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m, does not match size of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2569\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtarget_names, \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m. Try specifying the labels \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2570\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mparameter\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(labels), \u001b[39mlen\u001b[39m(target_names))\n\u001b[1;32m   2571\u001b[0m         )\n\u001b[1;32m   2572\u001b[0m \u001b[39mif\u001b[39;00m target_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2573\u001b[0m     target_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m l \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m labels]\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ...\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=encoder_polarity.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "843cc28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 48ms/step\n",
      "Polarity Precision: 0.3951\n",
      "Polarity Recall: 0.6286\n",
      "Polarity F1 Score: 0.4852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set for polarity\n",
    "y_pred_polarity = lstm_model_polarity.predict(X_test_polarity)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_polarity = np.argmax(y_pred_polarity, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_polarity = np.argmax(y_test_polarity, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for polarity\n",
    "precision_polarity = precision_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "recall_polarity = recall_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "f1_score_polarity = f1_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "\n",
    "print(f'Polarity Precision: {precision_polarity:.4f}')\n",
    "print(f'Polarity Recall: {recall_polarity:.4f}')\n",
    "print(f'Polarity F1 Score: {f1_score_polarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4e3a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 7ms/step\n",
      "Emotion Precision: 0.2359\n",
      "Emotion Recall: 0.4857\n",
      "Emotion F1 Score: 0.3176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set for emotion\n",
    "y_pred_emotion = lstm_model_emotion.predict(X_test_emotion)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_emotion = np.argmax(y_pred_emotion, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_emotion = np.argmax(y_test_emotion, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for emotion\n",
    "precision_emotion = precision_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "recall_emotion = recall_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "f1_score_emotion = f1_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "\n",
    "print(f'Emotion Precision: {precision_emotion:.4f}')\n",
    "print(f'Emotion Recall: {recall_emotion:.4f}')\n",
    "print(f'Emotion F1 Score: {f1_score_emotion:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba9a6d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 1s 936ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Tweet: I'm angry\n",
      "Polarity Label: neutral\n",
      "Emotion Label: happy\n",
      "Intensity Level: Low\n"
     ]
    }
   ],
   "source": [
    "# Assuming `polarity_labels` is your list of original labels for the training data\n",
    "# Example list of polarity labels used in your training dataset\n",
    "polarity_labels = ['positive', 'negative', 'neutral']  # This should be replaced by the actual labels you have\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "encoder_polarity = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder with your actual labels\n",
    "encoder_polarity.fit(polarity_labels)\n",
    "\n",
    "# Assuming encoder is a pre-defined LabelEncoder object for decoding the emotion labels\n",
    "\n",
    "# This function should extract LSTM features\n",
    "def extract_features(model, sequence):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    return intermediate_layer_model.predict(sequence)\n",
    "\n",
    "# Function to classify the intensity\n",
    "def classify_intensity(emoticons_count, text):\n",
    "    question_marks = text.count('?')\n",
    "    periods = text.count('.')\n",
    "    exclamation_marks = text.count('!')\n",
    "\n",
    "    if exclamation_marks > 1 or question_marks > 1 or emoticons_count > 1:\n",
    "        return 'High'\n",
    "    elif periods == 1 or question_marks == 1 or emoticons_count == 1 or exclamation_marks ==1 :\n",
    "        return 'Medium'\n",
    "    elif question_marks == 0 and emoticons_count == 0:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Undetermined'\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "    return ' '.join(tokenizer.tokenize(text))\n",
    "\n",
    "#Function to perform real-time prediction and intensity classification\n",
    "def real_time_prediction(text, tokenizer, lstm_model_emotion, lstm_model_polarity, svm_classifier_emotion, svm_classifier_polarity, encoder_emotion, encoder_polarity):\n",
    "    # Preprocessing steps (assuming these functions are defined elsewhere in your code)\n",
    "\n",
    "\n",
    "    cleaned_text = cleaning_numbers(text)\n",
    "    cleaned_tweet = clean_tweet(cleaned_text)\n",
    "    corrected_text = spell_correction(cleaned_tweet)\n",
    "    emoticon_converted_text, emoticons_count = convert_emoticons_to_words(corrected_text)  # Ensure this function returns emoticons_count\n",
    "    cleaned_stopwords = cleaning_stopwords(emoticon_converted_text)\n",
    "    cleaned_repeating_words = cleaning_repeating_words(cleaned_stopwords)\n",
    "\n",
    "    # Now tokenize the text after cleaning repeating words\n",
    "    tokenized_text = tokenize_text(cleaned_repeating_words)\n",
    "\n",
    "    # Continue with any additional preprocessing steps that work on the tokenized text\n",
    "    stemmed_text = stemming_on_text(tokenized_text)\n",
    "    lemmatized_text = lemmatizer_on_text(stemmed_text)\n",
    "\n",
    "    assert isinstance(lemmatized_text, str), \"Processed text must be a string\"\n",
    "\n",
    "    # Convert the processed text to a sequence\n",
    "    sequence = tokenizer.texts_to_sequences([lemmatized_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=50)\n",
    "\n",
    "\n",
    "    # Predict emotion and polarity using the LSTM model\n",
    "    lstm_prediction_emotion = lstm_model_emotion.predict(padded_sequence)\n",
    "    lstm_features_emotion = extract_features(lstm_model_emotion, padded_sequence)\n",
    "    svm_prediction_emotion = svm_classifier_emotion.predict_proba(lstm_features_emotion)\n",
    "\n",
    "    lstm_prediction_polarity = lstm_model_polarity.predict(padded_sequence)\n",
    "    lstm_features_polarity = extract_features(lstm_model_polarity, padded_sequence)\n",
    "    svm_prediction_polarity = svm_classifier_polarity.predict_proba(lstm_features_polarity)\n",
    "\n",
    "    # Decode the predicted labels\n",
    "    emotion_label = encoder_emotion.inverse_transform(np.argmax(lstm_prediction_emotion, axis=1))\n",
    "    polarity_label = encoder_polarity.inverse_transform(np.argmax(svm_prediction_polarity, axis=1))\n",
    "\n",
    "    # Get probabilities for the predicted labels\n",
    "    emotion_probability = np.max(lstm_prediction_emotion, axis=1)\n",
    "    polarity_probability = np.max(svm_prediction_polarity, axis=1)\n",
    "\n",
    "    # Classify the intensity\n",
    "    intensity = classify_intensity(emoticons_count, text)  # Ensure `emoticons_count` is defined\n",
    "\n",
    "    return polarity_label, emotion_label, polarity_probability, emotion_probability, intensity\n",
    "\n",
    "#This is the real time tweets\n",
    "tweet = \"I'm angry\"    \n",
    "# Call the real-time prediction function\n",
    "polarity_label, emotion_label, polarity_probability, emotion_probability, intensity = real_time_prediction(tweet, tokenizer, lstm_model_emotion, lstm_model_polarity, svm_classifier_emotion, svm_classifier_polarity, encoder_emotion, encoder_polarity)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Tweet: {tweet}\")\n",
    "print(f\"Polarity Label: {polarity_label[0]}\")\n",
    "print(f\"Emotion Label: {emotion_label[0]}\")\n",
    "print(f\"Intensity Level: {intensity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

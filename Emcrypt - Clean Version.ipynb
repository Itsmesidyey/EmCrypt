{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2a48e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#SpellCorrection\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import string\n",
    "import emoji\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7ec4dfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: UTF-8-SIG\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>5:52 PM · Oct 25, 2023</td>\n",
       "      <td>@jojomemecoin</td>\n",
       "      <td>Let crypto be the new global currency! 👑🤙\\n\\n#...</td>\n",
       "      <td>1</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>10/30/23</td>\n",
       "      <td>@JayFinn07</td>\n",
       "      <td>#Luffy and his crew is about to go on a voyage...</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>10/29/23</td>\n",
       "      <td>@TFutureguardian</td>\n",
       "      <td>SGX, the cutting-edge decentralized perpetual ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>10/29/23</td>\n",
       "      <td>@truongivie</td>\n",
       "      <td>29/10/2023 Quiz: \"Blockchain Trilemma\" #OverPr...</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>4:36 PM · Oct 25, 2023\\n</td>\n",
       "      <td>@UniverseCrypto7</td>\n",
       "      <td>$Scrt has confirmed its breakout from falling ...</td>\n",
       "      <td>1</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date          username  \\\n",
       "212    5:52 PM · Oct 25, 2023     @jojomemecoin   \n",
       "591                  10/30/23        @JayFinn07   \n",
       "466                  10/29/23  @TFutureguardian   \n",
       "334                  10/29/23       @truongivie   \n",
       "236  4:36 PM · Oct 25, 2023\\n  @UniverseCrypto7   \n",
       "\n",
       "                                                  text  polarity       emotion  \n",
       "212  Let crypto be the new global currency! 👑🤙\\n\\n#...         1  anticipation  \n",
       "591  #Luffy and his crew is about to go on a voyage...         1         Happy  \n",
       "466  SGX, the cutting-edge decentralized perpetual ...         1         Happy  \n",
       "334  29/10/2023 Quiz: \"Blockchain Trilemma\" #OverPr...         0       Neutral  \n",
       "236  $Scrt has confirmed its breakout from falling ...         1  anticipation  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['date', 'username', 'text', 'polarity', 'emotion']\n",
    "\n",
    "#Detect file encoding using chardet\n",
    "with open('data.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "\n",
    "# Print the detected encoding\n",
    "print(\"Detected encoding:\", result['encoding'])\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "df = pd.read_csv('data.csv', encoding=result['encoding'], names=DATASET_COLUMNS)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "10f5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "data=df[['text','polarity', 'emotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ed21aa48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2ba51c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['polarity'] == 2]\n",
    "data_neu = data[data['polarity'] == 1]\n",
    "data_neg = data[data['polarity'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b6d97b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data_pos.iloc[:int(100)]\n",
    "data_neu = data_neu.iloc[:int(100)]\n",
    "data_neg = data_neg.iloc[:int(100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0888a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([data_pos, data_neu, data_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b8bd68dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582     Which #crypto project has strong community? 💪🏼🔥🚀\n",
       "584    New Zealand, Rapper Sesh and DogeCoin Milliona...\n",
       "585    The founder of the bankrupt cryptocurrency exc...\n",
       "586    Unlock the Future with .mmit Domains! Join ove...\n",
       "595    If you sleep now, you will have a dream but if...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', '', data)\n",
    "dataset['text'] = dataset['text'].apply(lambda x: cleaning_numbers(x))\n",
    "dataset['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3469fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1            HAY bullflag breakout👀 Lets fill that wick🚀\n",
      "2      Did you guys see how is doing a pitch with a d...\n",
      "3      GN Fam going early to bed been up since or AM ...\n",
      "4      You think this week has been fun?!? 😂😂😂😂 Wait ...\n",
      "                             ...                        \n",
      "582                Which project has strong community? 🚀\n",
      "584    New Zealand Rapper Sesh and DogeCoin Millionai...\n",
      "585    The founder of the bankrupt cryptocurrency exc...\n",
      "586    Unlock the Future with .mmit Domains! Join ove...\n",
      "595    If you sleep now you will have a dream but if ...\n",
      "Name: text, Length: 197, dtype: object\n"
     ]
    }
   ],
   "source": [
    "emoticons_to_keep = [\n",
    "    '💰', '📈', '🤣', '🎊', '😂', '😭', '🙁', '😞', '💔', '😢', '😮', '😵', '🙀',\n",
    "    '😱', '❗', '😠', '😡', '😤', '👎', '🔪', '🌕', '🚀', '💎', '👀', '💭', '📉',\n",
    "    '😨', '😩', '😰', '💸'\n",
    "]\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove hashtags and mentions\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # Remove special characters except for emoticons\n",
    "    text = re.sub(r'[^\\w\\s.!?{}]+'.format(''.join(emoticons_to_keep)), '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the modified cleaning function to the 'text' column in your dataset\n",
    "dataset['text'] = dataset['text'].apply(clean_tweet)\n",
    "\n",
    "# Display the 'text' column in the entire dataset\n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9db4387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  polarity       emotion\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...         1         happy\n",
      "1            HAY bullfrog breakout Lets fill that wick         1  anticipation\n",
      "2    Did you guys see how is doing a pitch with a d...         1         happy\n",
      "3    GN Fam going early to bed been up since or AM ...         1         happy\n",
      "4    You think this week has been fun?!? 😂😂😂😂 Wait ...         1  anticipation\n",
      "..                                                 ...       ...           ...\n",
      "582               Which project has strong community i         0       Neutral\n",
      "584  New Zealand Rapper Sesh and DogeCoin Millionai...         0       Neutral\n",
      "585  The founder of the bankrupt cryptocurrency exc...         0       Neutral\n",
      "586  Unlock the Future with emmit Domains! Join ove...         0       Neutral\n",
      "595  If you sleep now you will have a dream but if ...         0       Neutral\n",
      "\n",
      "[197 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize SpellChecker only once to avoid re-creation for each call\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function for spell correction\n",
    "def spell_correction(text):\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        if word in misspelled:\n",
    "            corrected_word = spell.correction(word)\n",
    "            # Check if the correction is not None, otherwise use the original word\n",
    "            corrected_words.append(corrected_word if corrected_word is not None else word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Apply spell correction to the entire 'text' column\n",
    "dataset['text'] = dataset['text'].apply(spell_correction)\n",
    "\n",
    "# Display the entire dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "52520eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoticons converted to words in 'converted_text' column.\n",
      "                                      converted_text  emoticons_count\n",
      "0  BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...                0\n",
      "1          HAY bullfrog breakout Lets fill that wick                0\n",
      "2  Did you guys see how is doing a pitch with a d...                0\n",
      "3  GN Fam going early to bed been up since or AM ...                0\n",
      "4  You think this week has been fun?!? face with ...               13\n"
     ]
    }
   ],
   "source": [
    "#Define the emoticon dictionary outside the function for a wider scope\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smile \",\n",
    "    \":(\": \"sad \",\n",
    "    \":D\": \"laugh \",\n",
    "    \"😊\": \"smiling face with smiling eyes \",\n",
    "    \"😃\": \"grinning face with big eyes \",\n",
    "    \"😉\": \"winking face \",\n",
    "    \"👌\": \"OK hand \",\n",
    "    \"👍\": \"Thumbs up \",\n",
    "    \"😁\": \"beaming face with smiling eyes \",\n",
    "    \"😂\": \"face with tears of joy \",\n",
    "    \"😄\": \"grinning face with smiling eyes \",\n",
    "    \"😅\": \"grinning face with sweat \",\n",
    "    \"😆\": \"grinning squinting face \",\n",
    "    \"😇\": \"smiling face with halo \",\n",
    "    \"😞\": \"disappointed face \",\n",
    "    \"😔\": \"pensive face \",\n",
    "    \"😑\": \"expressionless face \",\n",
    "    \"😒\": \"unamused face \",\n",
    "    \"😓\": \"downcast face with sweat \",\n",
    "    \"😕\": \"confused face \",\n",
    "    \"😖\": \"confounded face \",\n",
    "    \"💰\": \"Money Bag \",\n",
    "    \"📈\": \"Up Trend \",\n",
    "    \"🤣\": \"Rolling on the Floor Laughing \",\n",
    "    \"🎊\": \"Confetti Ball \",\n",
    "    \"😭\": \"Loudly Crying \",\n",
    "    \"🙁\": \"Slightly frowning face \",\n",
    "    \"💔\": \"Broken Heart \",\n",
    "    \"😢\": \"Crying Face \",\n",
    "    \"😮\": \"Face with Open Mouth \",\n",
    "    \"😵\": \"Dizzy Face \",\n",
    "    \"🙀\": \"Weary Cat \",\n",
    "    \"😱\": \"Face Screaming in Fear \",\n",
    "    \"❗\": \"Exclamation Mark \",\n",
    "    \"😠\": \"Angry Face \",\n",
    "    \"😡\": \"Pouting Face \",\n",
    "    \"😤\": \"Face with Steam from Nose \",\n",
    "    \"👎\": \"Thumbs Down \",\n",
    "    \"🔪\": \"Hocho \",\n",
    "    \"🌕\": \"Moon \",\n",
    "    \"🚀\": \"Rocket \",\n",
    "    \"💎\": \"Diamond \",\n",
    "    \"👀\": \"Eyes \",\n",
    "    \"💭\": \"Thought Balloon \",\n",
    "    \"📉\": \"Down Trend \",\n",
    "    \"😨\": \"Fearful Face \",\n",
    "    \"😩\": \"Weary Face \",\n",
    "    \"😰\": \"Anxious Face with Fear \",\n",
    "    \"💸\": \"Money with Wings \"\n",
    "}\n",
    "\n",
    "# Emoticon to word conversion function\n",
    "def convert_emoticons_to_words(text):\n",
    "    changed_emoticons = 0  # Variable to count the number of changed emoticons\n",
    "    for emoticon, word in emoticon_dict.items():\n",
    "        while emoticon in text:\n",
    "            text = text.replace(emoticon, word + \" \", 1)\n",
    "            changed_emoticons += 1\n",
    "    return text, changed_emoticons\n",
    "\n",
    "# Apply the function and count emoticons for each row\n",
    "def apply_conversion(text):\n",
    "    converted_text, count = convert_emoticons_to_words(text)\n",
    "    return pd.Series([converted_text, count], index=['converted_text', 'emoticons_count'])\n",
    "\n",
    "conversion_results = dataset['text'].apply(apply_conversion)\n",
    "dataset['converted_text'] = conversion_results['converted_text']\n",
    "dataset['emoticons_count'] = conversion_results['emoticons_count']\n",
    "print(\"Emoticons converted to words in 'converted_text' column.\")\n",
    "print(dataset[['converted_text', 'emoticons_count']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c647fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ff9d183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stopwords removal applied separately after the option has been chosen and processed\n",
    "STOPWORDS = set(stopwordlist)\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# Apply the stopwords cleaning after the loop, once the 'text' column has been updated accordingly\n",
    "dataset['text'] = dataset['converted_text'].apply(cleaning_stopwords)\n",
    "print(\"Stopwords removed from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a9069e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeating words cleaned from 'text' column.\n",
      "0    BTC ON GLP RESISTANCE FOR NOW PLAY SAFE IF U R...\n",
      "1                 HAY bullfrog breakout Lets fill wick\n",
      "2    Did guys see pitch deck reaching community Tha...\n",
      "3    GN Fam going early bed since AM morning nonsto...\n",
      "4    You think week fun?!? face tears joy face tear...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to clean repeating words\n",
    "def cleaning_repeating_words(text):\n",
    "    # This regex pattern targets whole words that are repeated\n",
    "    return re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "\n",
    "# Assuming 'dataset' is a pandas DataFrame and 'text' is a column in it\n",
    "# Apply the cleaning function for repeating words to each row in the 'text' column\n",
    "dataset['text'] = dataset['text'].apply(cleaning_repeating_words)\n",
    "print(\"Repeating words cleaned from 'text' column.\")\n",
    "print(dataset['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7e1f0bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    btc on glp resistance for now play safe if u r...\n",
       "1                 hay bullfrog breakout lets fill wick\n",
       "2    did guys see pitch deck reaching community tha...\n",
       "3    gn fam going early bed since am morning nonsto...\n",
       "4    you think week fun?!? face tears joy face tear...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text']=dataset['text'].str.lower()\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "92c75347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# The pattern matches word characters (\\w) and punctuation marks ([^\\w\\s])\n",
    "tokenizer = RegexpTokenizer(r'\\w+|[^\\w\\s]')\n",
    "\n",
    "# Applying the modified tokenizer to the dataset\n",
    "dataset['text'] = dataset['text'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "dataset['text'] = dataset['text'].apply(tokenizer.tokenize)\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f63a0715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "dataset['text']= dataset['text'].apply(lambda x: stemming_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d0802f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [btc, on, glp, resistance, for, now, play, saf...\n",
       "1          [hay, bullfrog, breakout, lets, fill, wick]\n",
       "2    [did, guys, see, pitch, deck, reaching, commun...\n",
       "3    [gn, fam, going, early, bed, since, am, mornin...\n",
       "4    [you, think, week, fun, ?, !, ?, face, tears, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "dataset['text'] = dataset['text'].apply(lambda x: lemmatizer_on_text(x))\n",
    "dataset['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ffd9e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 3s 41ms/step - loss: 0.6911 - accuracy: 0.5032\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 0.6723 - accuracy: 0.6688\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 0.6557 - accuracy: 0.6752\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 0.6173 - accuracy: 0.8025\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 0.5531 - accuracy: 0.8726\n",
      "Epoch 1/5\n",
      "5/5 [==============================] - 3s 37ms/step - loss: 1.7826 - accuracy: 0.3121\n",
      "Epoch 2/5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 1.7290 - accuracy: 0.4968\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 0s 35ms/step - loss: 1.6379 - accuracy: 0.4904\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 0s 36ms/step - loss: 1.4472 - accuracy: 0.4395\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 1.3369 - accuracy: 0.4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# All necessary imports should be placed at the top of the script\n",
    "from keras.models import Sequential, Model, save_model\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Initialize the tokenizer with your dataset\n",
    "# NOTE: Ensure `dataset` is correctly loaded with 'text' column\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['text'])\n",
    "\n",
    "# Function to create LSTM model\n",
    "def create_lstm_model(input_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to extract features from LSTM model\n",
    "def extract_features(model, data):\n",
    "    intermediate_layer_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "    return intermediate_layer_model.predict(data)\n",
    "\n",
    "# Assuming `dataset` is a pandas DataFrame with 'text', 'polarity', and 'emotion' columns\n",
    "sequences = tokenizer.texts_to_sequences(dataset['text'])\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "# Convert polarity and emotion labels to one-hot encoding\n",
    "encoder_polarity = LabelEncoder()\n",
    "y_polarity = to_categorical(encoder_polarity.fit_transform(dataset['polarity']))\n",
    "\n",
    "encoder_emotion = LabelEncoder()\n",
    "y_emotion = to_categorical(encoder_emotion.fit_transform(dataset['emotion']))\n",
    "\n",
    "# Split the data into training and testing sets for polarity and emotion\n",
    "X_train_polarity, X_test_polarity, y_train_polarity, y_test_polarity = train_test_split(X, y_polarity, test_size=0.2, random_state=42)\n",
    "X_train_emotion, X_test_emotion, y_train_emotion, y_test_emotion = train_test_split(X, y_emotion, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train LSTM models for polarity and emotion\n",
    "lstm_model_polarity = create_lstm_model(50, y_polarity.shape[1])\n",
    "lstm_model_emotion = create_lstm_model(50, y_emotion.shape[1])\n",
    "\n",
    "lstm_model_polarity.fit(X_train_polarity, y_train_polarity, epochs=5, batch_size=32)\n",
    "lstm_model_emotion.fit(X_train_emotion, y_train_emotion, epochs=5, batch_size=32)\n",
    "\n",
    "# Save the trained LSTM models to H5 files\n",
    "lstm_model_polarity.save('lstm_polarity_model.h5')\n",
    "lstm_model_emotion.save('lstm_emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dfcb89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 8ms/step\n",
      "5/5 [==============================] - 1s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features with LSTM for SVM training\n",
    "X_train_features_polarity = extract_features(lstm_model_polarity, X_train_polarity)\n",
    "X_train_features_emotion = extract_features(lstm_model_emotion, X_train_emotion)\n",
    "\n",
    "# Train SVM for polarity and emotion\n",
    "svm_classifier_polarity = SVC(kernel='linear', probability=True)\n",
    "svm_classifier_emotion = SVC(kernel='linear', probability=True)\n",
    "\n",
    "svm_classifier_polarity.fit(X_train_features_polarity, np.argmax(y_train_polarity, axis=1))\n",
    "svm_classifier_emotion.fit(X_train_features_emotion, np.argmax(y_train_emotion, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "45cf3e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_emotion_model.joblib']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained SVM models using joblib or pickle\n",
    "import joblib\n",
    "joblib.dump(svm_classifier_polarity, 'svm_polarity_model.joblib')\n",
    "joblib.dump(svm_classifier_emotion, 'svm_emotion_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "843cc28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 8ms/step\n",
      "Polarity Precision: 0.7240\n",
      "Polarity Recall: 0.7000\n",
      "Polarity F1 Score: 0.6955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions on the test set for polarity\n",
    "y_pred_polarity = lstm_model_polarity.predict(X_test_polarity)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_polarity = np.argmax(y_pred_polarity, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_polarity = np.argmax(y_test_polarity, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for polarity\n",
    "precision_polarity = precision_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "recall_polarity = recall_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "f1_score_polarity = f1_score(y_true_polarity, y_pred_polarity, average='weighted')\n",
    "\n",
    "print(f'Polarity Precision: {precision_polarity:.4f}')\n",
    "print(f'Polarity Recall: {recall_polarity:.4f}')\n",
    "print(f'Polarity F1 Score: {f1_score_polarity:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b4e3a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 10ms/step\n",
      "Emotion Precision: 0.4891\n",
      "Emotion Recall: 0.4250\n",
      "Emotion F1 Score: 0.2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set for emotion\n",
    "y_pred_emotion = lstm_model_emotion.predict(X_test_emotion)\n",
    "# Convert predictions from one-hot encoded to label encoded for evaluation\n",
    "y_pred_emotion = np.argmax(y_pred_emotion, axis=1)\n",
    "# Convert ground truth from one-hot encoded to label encoded for evaluation\n",
    "y_true_emotion = np.argmax(y_test_emotion, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and F-measure for emotion\n",
    "precision_emotion = precision_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "recall_emotion = recall_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "f1_score_emotion = f1_score(y_true_emotion, y_pred_emotion, average='weighted')\n",
    "\n",
    "print(f'Emotion Precision: {precision_emotion:.4f}')\n",
    "print(f'Emotion Recall: {recall_emotion:.4f}')\n",
    "print(f'Emotion F1 Score: {f1_score_emotion:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
